\documentclass[english,bibtotoc,liststotoc,oneside,BCOR=5mm,DIV=12]{scrbook}
\recalctypearea

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{url}
\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{positioning,fit,arrows.meta,backgrounds}
\usepackage{adjustbox} % für max width = \linewidth
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{csvsimple}   % moderne Version mit \csvautotabular
\usepackage{booktabs}
\usepackage{adjustbox}
\pgfplotsset{compat=1.18}
\usepackage{float}
\usepackage{listings, color}
\usepackage{subcaption}
\usetikzlibrary{positioning,arrows.meta}
\usepackage[automark]{scrlayer-scrpage}
\setlength{\marginparwidth}{2cm}
\usepackage{todonotes}
\usepackage{comment}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,backgrounds,shadows,decorations.pathmorphing,decorations.pathreplacing}
\usetikzlibrary{calc,intersections,through,backgrounds,matrix}
\usetikzlibrary{arrows.meta, calc, positioning, shapes.geometric}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{array}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\usepackage[table]{xcolor}
\usepackage[most]{tcolorbox}
\tcbuselibrary{listings,skins}
% Neutral panel with teal accent for algorithms/notes
\newtcolorbox{algwithnotes}[1][]{enhanced,breakable,sharp corners,boxrule=0.6pt,
  colback=blue!3!white,colframe=black!20,borderline west={2pt}{0pt}{teal!70!black},
  title={#1},fonttitle=\bfseries}

% Listings style for nicer code blocks
\lstdefinestyle{codeblock}{%
  basicstyle=\ttfamily\small,
  breaklines=true,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  frame=single,
  framerule=0.5pt,
  backgroundcolor=\color{blue!5!white},
  rulecolor=\color{teal!70!black},
  numbers=left,
  numberstyle=\tiny,
  numbersep=16pt,
  xleftmargin=2.6em,
  framexleftmargin=2.0em
}
\lstset{style=codeblock}

% Theorem-like environments
\newtheorem{definition}{Definition}

\usepackage{booktabs}
\usepackage{csquotes}
\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{bib/references.bib}

% Only number and list up to sections (no subsection/subsubsection numbering in ToC or headings)
\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}

\clearpairofpagestyles
\chead[Verifiable Data Transformations in IoT Environments – Ramón Felipe Kühne]{Verifiable Data Transformations in IoT Environments – Ramón Felipe Kühne}
% Unified footer for all page styles (plain and scrheadings)
\cfoot[TU Berlin, 2025 \quad | \quad \thepage]{TU Berlin, 2025 \quad | \quad \thepage}
\KOMAoptions{headsepline=.4pt}
\renewcommand*{\chapterpagestyle}{scrheadings}
\pagestyle{scrheadings}

\graphicspath{{./img/}{../data/visualizations/}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\frontmatter
\input{misc/titlepage}
    \thispagestyle{empty}
    \cleardoublepage
    
\input{misc/self-assertion}
    \thispagestyle{empty}
    \cleardoublepage

% Switch to arabic numbering for all following pages
\mainmatter
\pagenumbering{arabic}
\setcounter{page}{1}

\section*{Declaration of Authorship}
I hereby declare that I have written this thesis independently and have not used any sources or aids other than those stated. All passages taken directly or indirectly from the published or unpublished work of others have been identified as such. This thesis has not been submitted in substantially the same form to any other examination board and has not been published.

\clearpage

\section*{Abstract}
This thesis evaluates standard and recursive zero knowledge proof pipelines for verifiable processing of Internet of Things sensor data under constrained edge resources. The system allows a consumer to validate range checks and aggregate properties without access to raw measurements. A role based architecture authenticates devices with certificates, verifies signatures at the edge and generates proofs while the consumer verifies a single artifact. We compare a standard Groth16 pipeline against a recursive Nova pipeline and study the effect of step size and input size on end to end time and proof size. Proof size for the recursive pipeline remains essentially constant while the standard pipeline grows linearly. With a step size of ten the time crossover occurs at four hundred readings and the advantage grows with the input size. A non zero knowledge baseline provides a lower bound for runtime and size and shows that the dominant costs arise from proof generation and verification rather than the application level computation. The results yield actionable guidance for choosing step sizes and proof schemes on edge devices.
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgements}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\clearpage
\cleardoublepage
\listoffigures
\clearpage
\listoftables
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{sec:intro}

\section{Motivation}
\label{sec:motivation}
IoT deployments such as smart home sensors, industrial monitors, and environmental sensing networks generate continuous high resolution time series data. To reduce communication and storage demands on resource constrained edge devices, this data is often aggregated in batches, for example via summation or averaging. Conventional aggregation lacks formal guarantees regarding data integrity and privacy \cite{SmartMeterPrivacySurvey2023, AStudyOnPrivacyPreserving2021}. Research has shown that even coarse patterns like hourly energy usage can expose private habits \cite{muellerGewinnungVerhaltensprofilenAm2010}. Aggregation pipelines that rely on unverified local computation are susceptible to tampering or omission, which undermines trust in reported values \cite{bohliPrivacyModelSmart2010aa}. This combination of emerging privacy threats and trust issues highlights the need for cryptographic verification mechanisms in IoT aggregation systems.

Global data production has exploded over the past decade. In 2010 approximately 2 zettabytes of data existed. By 2023 that number reached about 120 zettabytes. In 2024 it rose to approximately 147 zettabytes. Projections expect global data volume to grow further to 181 zettabytes by 2025 \cite{IDCExpect175, HowMuchData}. This dramatic increase magnifies the potential impact of large scale data breaches. In the healthcare sector alone the number of breaches reported to U.S. authorities reached 725 in 2023, exposing over 133 million records \cite{alderDecember2023Healthcare2024}.

The proliferation of IoT devices further accelerates data generation and increases privacy risk. Smart thermostats, smart meters, wearable health trackers, voice assistants, and environmental sensors continuously collect sensor data, often without users’ full awareness. These devices shape daily life and generate intimate behavioral insights.

Because massive volumes of data are generated every moment and breaches are escalating, ensuring the integrity and confidentiality of aggregated data has become critically important. This motivates the development of cryptographic methods that can verify aggregated IoT data without compromising user privacy.

\section{Problem Statement}
\label{sec:problemstatement}
The central problem of this thesis has two main aspects. First, existing aggregation methods do not provide formal integrity guarantees. In practice, users cannot confirm that published aggregates include all raw sensor readings nor detect whether any data were omitted or modified during processing. Second, although zkSNARKs allow confidential proofs of correctness, classical non recursive approaches become increasingly inefficient when used repeatedly for continuous streaming data. The core inefficiency stems from computational complexity, especially during witness generation, which can easily become a bottleneck on IoT hardware with limited resources \cite{ElHajj2024BenchmarkStudy}. In addition, many traditional zkSNARK protocols depend on a trusted setup and do not allow parallel processing, which limits their scalability in IoT use cases.

Recursive zkSNARKs present a promising alternative. They support proof chaining across batches, so that verification cost is amortized rather than repeated. Recent systems such as GENES demonstrate substantial improvements in proving time and verification latency through recursive proof composition. However, these improvements sometimes come with the trade off of larger overall proof sizes \cite{Genes2025EfficientRecursiveSnark}. Likewise, Zecale demonstrates how recursive aggregation can substantially reduce verification overhead while preserving privacy in blockchain contexts \cite{Rondelet2020Zecale}. Despite these theoretical advantages, the deployment of recursive zkSNARKs in constrained, privacy critical IoT environments has not yet been evaluated.

This research therefore targets a gap in current understanding by empirically establishing when recursive zkSNARKs offer a measurable advantage compared to classical zkSNARKs under realistic IoT conditions (hardware limits, privacy objectives, communication constraints). Our benchmarks compare recursive systems to non‑recursive implementations using identical data and report only measured latency, memory, and proof‑size results to produce actionable guidance for real‑world system designers.

\section{Research Questions}

Our research is guided by the following primary questions:

\begin{enumerate}
    \item Under which conditions is the use of recursive SNARKs beneficial?
    \item What added value do recursive SNARKs provide in the context of privacy?
    \item From which data volume or computational complexity onwards are recursive SNARKs more efficient?
    \item Can empirical measurements on realistic IoT setups determine when recursion becomes advantageous?
    \item What are the privacy-performance trade-offs in recursive vs. standard SNARK systems?
\end{enumerate}

\section{Contributions}

This thesis makes the following key contributions:

\begin{enumerate}
    \item \textbf{Nova Implementation}: Complete implementation of Nova recursive SNARKs optimized for IoT data processing
    \item \textbf{Empirical Validation}: Comprehensive resource-constrained IoT evaluation
    \item \textbf{Performance Analysis}: Detailed benchmarking across multiple scenarios
    \item \textbf{Practical Guidelines}: Decision frameworks for choosing appropriate proof systems
\end{enumerate}

\section{Thesis Structure}

\noindent The thesis is organized into eight chapters that map directly to the research questions and the evaluation pipeline:
\begin{enumerate}
    \item \textbf{Introduction} (\cref{sec:intro}): Motivation, problem statement, research questions, contributions, and chapter roadmap.
    \item \textbf{Background} (\cref{sec:background}): Task-relevant overview of IoT aggregation privacy risks and zero-knowledge systems; emphasis on concepts required to interpret the later crossover analysis.
    \item \textbf{Related Work} (\cref{chap:related-work}): Positioning within IoT privacy aggregation and zero-knowledge literature; highlights the gap this thesis addresses; core concepts and trade-offs of recursive vs. non-recursive zk-SNARKs relevant to this study.
    \item \textbf{System Architecture} (\cref{chap:system-architecture}): End-to-end architecture and components; actors; data flow and threat model; how the project is structured and executed.
    \item \textbf{Implementation} (\cref{chap:implementation}): Pipelines, tooling, measurement harness, reproducibility, and limitations.
    \item \textbf{Empirical Results and Analysis} (\cref{chap:empirical-results}): Integrated results covering crossover validation, temporal batching, sensitivity analysis, device-level performance, and practical selection guidelines.
    \item \textbf{Discussion} (\cref{chap:discussion}): Interpretation of findings, decision framework for practitioners, and threats to validity.
    \item \textbf{Conclusion \& Future Work} (\cref{chap:conclusion}): Summary of contributions and directions for future research.
\end{enumerate}

\noindent This structure keeps the narrative focused on the central objective: reporting strictly empirical advantages of recursive SNARKs in IoT settings. Each part either introduces a necessary concept, contributes a component of the methodology, or reports measured results that directly answer the research questions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background}
\label{sec:background}
\section{Privacy \& Data Aggregation in IoT}
Resource constrained devices in Internet of Things environments collect and transmit sensor data such as temperature, power usage or motion events. Aggregating this data can reduce communication load and storage overhead, but doing so without cryptographic guarantees can compromise data integrity or privacy. A review by Ali et al \cite{InayatAliPrivacyPreservingDataAggregation2018} shows that traditional data aggregation techniques may expose raw readings and remain vulnerable to inference or tampering, especially in constrained sensor networks. Solutions such as LiPI \cite{GoyalLiPI2022} propose lightweight data masking mechanisms, but they often trade off integrity verification or depend on trusted components. There is limited research on cryptographically verifiable aggregation tailored for resource limited IoT nodes, especially when continuous privacy preservation is required.

\section{Overview of Zero-Knowledge Proofs}
A zero-knowledge proof (ZKP) lets a prover convince a verifier that a statement is true without revealing any additional information beyond its truth \cite{goldwasserKnowledgeComplexityInteractive1985}. ZKPs exist in interactive and non-interactive forms.
zk-SNARKs are non-interactive arguments of knowledge with succinct proofs; in many constructions, proof size and verifier work are sublinear—often effectively constant—in the size of the computation, though they may depend on public input size and typically require a setup \cite{nitulescuZkSnarksAGentleIntroduction2021}. zk-SNARKs have seen prominent deployments, e.g., in Zerocash/Zcash \cite{bensassonZerocashDecentralizedAnonymous2014,hopwoodZcashProtocolSpecification}.
zk-STARKs are transparent (no trusted setup) and hash-based; they scale well and are plausibly post-quantum, but usually incur larger proofs and higher prover costs \cite{ben-sassonScalableZeroKnowledge2019}.
Bulletproofs provide short non-interactive proofs without trusted setup with logarithmic proof size for certain statements (e.g., range proofs); however, verification is generally more expensive than in SNARK systems for large circuits \cite{BulletproofsStanfordApplied}.
Other families (e.g., Sonic, Plonk/Halo variants) explore different trade-offs in universality, transparency, setup, and efficiency \cite{mallerSonicZeroKnowledgeSNARKs2019}.
A recent survey overviews applications and practical frameworks across domains \cite{ASurveyApplicationsZKP2024}.

\section{Verifiable Transformations in IoT Environments}

Verifiable transformations in IoT environments refer to cryptographic computations that process sensor data while maintaining both privacy and computational integrity guarantees. These transformations enable the verification of data processing correctness without revealing individual sensor readings, making them essential for privacy-preserving IoT aggregation systems.

\subsection{Definition and Requirements}

A verifiable transformation in IoT contexts must satisfy three fundamental requirements: (i) \emph{computational integrity}, ensuring that published results represent correct computations over authentic input data; (ii) \emph{privacy preservation}, preventing the disclosure of individual sensor readings beyond what is logically implied by the output; and (iii) \emph{verification efficiency}, enabling efficient proof verification even on resource-constrained devices.

\subsection{Circuit Logic and Cryptographic Components}

The circuit logic for IoT verifiable transformations typically involves several cryptographic components. First, \emph{device signature verification} ensures that each sensor reading originates from an authenticated IoT device using the device's public key. Second, \emph{computational integrity verification} proves that the aggregation function was applied correctly to the verified inputs. The circuit design distinguishes between \emph{public arguments} (device public keys, aggregated results, timestamps) and \emph{private arguments} (individual sensor readings, device private keys, intermediate computation states).

\subsection{General Transformation Types}

Our implementation demonstrates several classes of verifiable transformations applicable to IoT environments:

\begin{itemize}
\item \textbf{Range Validation}: Proving that sensor readings fall within acceptable bounds without revealing individual values
\item \textbf{Statistical Aggregation}: Computing sums, means, medians, or other statistics while maintaining input privacy
\item \textbf{Threshold Compliance}: Verifying that aggregated metrics meet predefined thresholds without exposing individual contributions
\item \textbf{Temporal Analysis}: Proving properties about data collected over time windows while preserving temporal privacy
\end{itemize}

These transformations form the foundation for privacy-preserving IoT data processing, enabling verifiable analytics while protecting individual device privacy.

\subsection*{Non-ZK baseline in this thesis}
Non-cryptographic processing can be faster, yet it reveals raw inputs during checking or depends on trust in the computing entity, which conflicts with our privacy requirement. In this thesis a non-zero-knowledge baseline is used only as a reference to contextualize raw execution time and output size. It does not serve as an alternative solution because it cannot provide both confidentiality and verifiable computational integrity at the same time \cite{goldwasserKnowledgeComplexityInteractive1985,katzIntroductionModernCryptography2007}.

\section{Recursive ZKPs and Aggregation}
Recursive zero knowledge proofs stack or fold multiple proofs into a single succinct result. This enables efficient and scalable verification especially in streaming or multi step computation settings where multiple sub proofs are generated.

\subsection*{Definition of aggregation in this thesis}
We use the term \emph{aggregation} narrowly to denote computations over sets or windows of readings that reduce raw data to concise statistics or validity results (e.g. range validation, sum/mean/median, min/max). In our scope, privacy-preserving aggregation must (i) reveal nothing about individual readings beyond what is logically implied by the output, and (ii) enable verifiers to check correctness without access to raw inputs. These requirements motivate zero-knowledge approaches \cite{goldwasserKnowledgeComplexityInteractive1985,katzIntroductionModernCryptography2007} and connect directly to our circuits and pipeline in \cref{chap:implementation}.

\subsection{Principles and Benefits}
The core idea of recursive ZKPs is to verify a proof inside another proof, thus composing multiple statements into an incrementally verifiable chain. This approach is formalized in theories such as incrementally verifiable computation and proof folding schemes. Nova introduced an efficient folding scheme that absorbs complexity into a relaxed R1CS representation, dramatically reducing per proof cost while maintaining succinct final proofs \cite{NovaFoldingIVC2023}. This makes recursion especially powerful when many steps must be verified sequentially.

\subsection{Frameworks: Halo, Nova, Plonky2}
Halo, introduced by Bowe et al in 2019, pioneered recursive SNARK designs that do not require a trusted setup. It supports cycles of elliptic curves and recursive proof composition transparently \cite{boweRecursiveProofComposition2019}. Nova builds on similar ideas through an efficient folding based proof aggregation strategy and achieves state of the art performance in proof generation and succinctness \cite{NovaFoldingIVC2023, PantheonNovaBenchmark}. Plonky2 is a zk-STARK based system optimized by Polygon Zero for recursive workloads. It uses custom gates and deep arithmetic constraints to enable recursion at scale with high proving speed \cite{Plonky2ZKM2025, AnalysisPlonky2Protocol, IntroducingPlonky2, MayaZKBlogAggregationSummary}. All three systems allow continual chaining of proofs and compression into a single final proof, reducing verification overhead in multi step or streaming use cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Work}
\label{chap:related-work}

\section{IoT Privacy and Data Aggregation}

The overlap between IoT privacy preservation and data aggregation has been extensively studied. Traditional approaches to IoT data aggregation often sacrifice privacy for efficiency, creating vulnerabilities in smart home and industrial deployments \cite{SmartMeterPrivacySurvey2023, AStudyOnPrivacyPreserving2021}.

\subsection{Privacy-Preserving IoT Aggregation}

Early work by Ali et al. demonstrated that conventional aggregation techniques expose raw sensor readings to inference attacks \cite{InayatAliPrivacyPreservingDataAggregation2018}. Solutions such as LiPI propose lightweight obfuscation mechanisms but often trade off integrity verification or depend on trusted components \cite{GoyalLiPI2022}.

Recent advances in differential privacy for IoT have shown promise but struggle with the continuous, high-frequency nature of sensor data. The challenge lies in balancing privacy preservation with the computational and energy constraints of IoT devices. Prior work rarely offers end-to-end, verifiable aggregation with measured crossover points under resource constraints; our study fills this empirical gap by reporting only measured results and explicit crossover regimes.

\subsection*{Critical positioning}
Compared to prior surveys and systems, our contribution is explicitly empirical and hardware-aware: (i) we benchmark standard vs. recursive zk-SNARKs on identical logic and data under matched resource limits enforced via Docker (CPU/RAM constraints), and (ii) we report time/size crossovers under steady-state operation. Many prior works emphasize protocol design or transparency properties (e.g., STARKs) without quantifying when recursion is advantageous under constrained compute and memory. Our results provide that missing, practical boundary.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Recursive vs. Non-Recursive zk-SNARKs in Resource-Constrained Environments}
\label{sec:zk-snark-comparison}

\subsection{Fundamentals: Difference Between Recursive and Non-Recursive zk-SNARKs}

zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge) allow one to prove the correctness of a computation using a short cryptographic proof without revealing the underlying data. A non-recursive zk-SNARK refers to a single proof for a specific computation or statement. In contrast, recursive zk-SNARKs allow multiple proofs or computation steps to be composed into each other. In recursion, the output of one zk-SNARK is used as part of the input for the next, resulting in a single final proof that attests to the correctness of all intermediate computations \cite{innovationBlockchainScalabilityGuide}. This is also known as incrementally verifiable computation (IVC): the prover produces a proof for each computation step that confirms both the correctness of that step and that the previous step was correctly verified \cite{bellachiaVerifBFLLeveragingZkSNARKs2025}. Through this composition, iterative or sequential computations can be securely chained.

A well-known example of recursive zk-SNARKs is Nova, which is based on a folding scheme. Nova folds a long computation into an ongoing recursive proof and only generates the final zk-SNARK at the end \cite{ElHajj2024BenchmarkStudy}. As a result, the expensive zk-SNARK generation occurs only once—regardless of how many steps were involved in the computation. Systems such as Halo or Nova have demonstrated that recursive zk-SNARKs can be built without a trusted setup, making them suitable for real-world applications \cite{ElHajj2024BenchmarkStudy}.

\subsection{Efficiency, Computation Cost, and Latency}

The primary efficiency difference lies in the trade-off between proof generation and verification. In non-recursive zk-SNARKs, generating a single proof is expensive, but verifying that proof is very fast (often milliseconds). However, if multiple zk-SNARKs must be verified (e.g., many individual proofs), the overall verification time scales linearly. Recursive zk-SNARKs aim to drastically reduce this verification overhead by aggregating all claims into a single proof \cite{innovationBlockchainScalabilityGuide}. Thus, the final verification time remains essentially constant, regardless of the number of individual steps or proofs involved.

On the proving side, recursive SNARKs introduce some overhead, since each new proof must verify the previous one, increasing the number of constraints. In traditional constructions (e.g., Groth16), verifying a SNARK inside a SNARK was costly. Modern systems like Nova optimize this by delaying the expensive zk-SNARK compression to the end \cite{ElHajj2024BenchmarkStudy}. Nova works in two stages: it first builds an ongoing recursive proof and then applies a final zk-SNARK compression. This final step incurs a fixed cost, regardless of how many steps were folded in. Hence, the final verification time remains constant, while the proof generation time increases roughly linearly with the number of steps. Latency may increase moderately, since the system waits until the end to compress the accumulated proofs.

Proof size is another major advantage. While a typical Groth16 proof is constant in size, producing many individual proofs results in linear growth in storage or transmission. Recursive SNARKs produce one final compact proof whose size is largely independent of the number of inputs \cite{innovationBlockchainScalabilityGuide}.

\subsection{Scalability}
Recursive zk-SNARKs are most beneficial when dealing with large-scale computations or proof aggregation. For small or one-time computations, a single non-recursive proof is often more efficient, as the recursive overhead may not be justified.

Empirical studies indicate that even at modest batch sizes (a few dozen proofs), recursion can become advantageous. For example, in a decentralized IoT setting, Nova required only \textasciitilde3.6 seconds to aggregate and verify 10 digital signatures, whereas a non-recursive method using Risc0 took \textasciitilde369 seconds—over 100$\times$ slower \cite{bojicburgosDecentralizedIoTData2024}. The gap grows with more inputs. Another study showed that Nova could verify 100 signatures in 7.1 seconds, whereas a previous method based on homomorphic encryption and ECDSA took over 50 seconds to verify just 64 signatures \cite{bojicburgosDecentralizedIoTData2024}. These results suggest that at batch sizes of a few dozen, recursive approaches can already be significantly more efficient.

Moreover, recursion reduces distributed verification overhead. Without recursion, each verifier must check all proofs. With recursion, only a single final proof needs to be verified. This makes the per-claim verification time negligible, since a constant cost is amortized over many claims \cite{bojicburgosDecentralizedIoTData2024}. The load is shifted from weak verifiers (e.g., IoT devices or smart contracts) to a single strong prover.

\subsection{Use in IoT and Smart-Home Scenarios}

IoT and smart-home environments impose strict constraints: sensors and embedded devices often have limited processing power, memory, and energy. zk-SNARK generation is typically too expensive to perform locally \cite{bojicburgosDecentralizedIoTData2024}. Even verification can overwhelm constrained devices. Therefore, many architectures follow a layered model with edge servers.

In this setup, IoT devices only collect and sign data. They then forward it to a nearby edge aggregator, which performs proof generation and aggregation \cite{bojicburgosDecentralizedIoTData2024}. Only the final proof or its hash is sent to a blockchain or central verifier. This eliminates the need for IoT devices to generate or verify SNARKs, saving energy and bandwidth.

Recursive zk-SNARKs are ideal for such scenarios, as they can aggregate continuous sensor streams into an ongoing proof. For instance, Nova has been used to aggregate and verify 100 sensor signatures into a single proof suitable for on-chain verification \cite{bojicburgosDecentralizedIoTData2024}. Verifying this proof took only \textasciitilde0.06\,s per signature (i.e., \textasciitilde6\,s total), even for low-powered verifiers.

Beyond signature verification, recursive SNARKs can prove compliance with rules over long periods, such as “no sensor exceeded a threshold for the past hour.” This streaming proof model allows incremental updates and compact final validation, ideal for constrained environments \cite{ElHajj2024BenchmarkStudy}.

Studies have even demonstrated recursive zk-SNARKs in advanced tasks like federated learning: each local training round and the global aggregation step are provably verified using Nova. In one setup, the global model proof took \textasciitilde81 seconds to generate and \textasciitilde0.6 seconds to verify \cite{bellachiaVerifBFLLeveragingZkSNARKs2025}. This shows that the cost is mostly on the proving side, which can be offloaded to strong devices.

\subsection{Summary and Implications for Architecture}
Recursive zk-SNARKs offer compelling benefits for scaling zero-knowledge applications in IoT scenarios. They enable aggregation of multiple computations or data streams into a single compact proof, which reduces memory, bandwidth, and verification cost—key concerns in resource‑constrained environments. Our system architecture (\cref{chap:system-architecture}) therefore places proving at an edge aggregator, defines batching policies that drive into empirically observed crossover regimes, and adopts metrics (proof time, verification time, proof size, and device load) that operationalize these trade‑offs. The following chapter translates these implications into a concrete architecture and methodology and specifies the evaluation setup used to validate them in practice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{System Architecture}

\label{chap:system-architecture}

This chapter presents the system architecture for privacy-preserving verification of IoT computations. The goal is to enable verification of range checks and aggregate properties of sensor data without revealing the raw measurements. We use zk-SNARKs for standard proving and recursive zk-SNARKs for batch wise verification of many steps as a single succinct proof.

\begin{figure}[H]  % oder [!htb] wenn du es nicht strikt erzwingen willst
\centering
\resizebox{0.98\textwidth}{!}{%
\begin{tikzpicture}[
    node distance=2.2cm and 3.0cm,
    actor/.style={draw, rounded corners=8pt, align=center, minimum width=3.6cm, minimum height=0.95cm, fill=gray!8, font=\scriptsize},
    center/.style={actor, fill=blue!20, text=white, font=\bfseries\scriptsize},
    arr/.style={-{Stealth[length=2.2mm,width=1.2mm]}, thick}
  ]
  % Center
  \node[center] (edge) {Edge Orchestrator};
  
  % Ring
  \node[actor, above=of edge]            (pki)  {Key Registry};
  \node[actor, left=of edge]             (dev)  {IoT Device};
  \node[actor, right=of edge]            (prov) {Prover};
  \node[actor, below left=1.6cm and 1.8cm of edge] (ver)  {Verifier};
  \node[actor, below right=1.6cm and 1.8cm of edge] (cons) {Consumer};
  
  % Flows
  \draw[arr] (dev) -- node[above, font=\scriptsize]{sensor readings + signature} (edge);
  \draw[arr] (pki) -- node[right, font=\scriptsize]{device credential} (edge);
  \draw[arr] (edge) -- node[above, font=\scriptsize]{witness + policy} (prov);
  \draw[arr] (prov) -- node[above, font=\scriptsize]{standard/recursive proof} (ver);
  \draw[arr] (ver) -- node[above, font=\scriptsize]{verified result} (cons);
\end{tikzpicture}
}
  \caption{System architecture and interactions}
  \label{fig:usecase-architecture}
\end{figure}

\section{Actors and Trust Model}
The architecture includes six roles that interact as shown in Figure~\ref{fig:usecase-architecture}. The IoT Device generates sensor readings and signs each payload with a device key that never leaves the device. The Key Registry, acting as issuer and certification authority, issues a device credential that binds the device identity to its public key. The Edge Orchestrator receives sensor readings and signatures, verifies the certificate and each signature, and prepares the public policy and the private witness for proving. The Prover produces a proof for the prepared statement, either as a standard zk-SNARK proof or as a recursive zk-SNARK proof. The Verifier checks the received proof with the verification key, and the Consumer receives only a verified result. We assume the certification authority is trusted by the Edge Orchestrator and the Verifier, the device private key remains on the device, the device credential is issued by the certification authority, the proving key stays with the Orchestrator, and the verification key is distributed to verifiers.

\section{Key Management}
Device identity is established by generating the keypair on the device and obtaining a credential issued by the certification authority. In our evaluation a local certification authority is created to keep runs reproducible and the resulting artifacts are managed by the orchestration environment. For the proof system the proving key is created locally by the orchestration role during setup and it is not shared. The verification key is exported and provided to the verification role for proof checking.

\section{Data and Control Flow}
Data and control follow the simple pattern shown in Figure~\ref{fig:usecase-architecture}. The IoT Device emits sensor readings together with a signature. The certification authority provides an authority root and a device credential to enable authentication. The Edge Orchestrator verifies both, prepares public policy and private witness, and triggers proving. The Prover returns either a standard proof or, if recursion is used, a compressed proof. The Verifier checks the proof with the verification key and the Consumer receives only a verified result.

\section{Verifiable Transformations}
We define a general transformation \(f\) that processes private readings under a public policy. Public inputs encode the policy and minimal metadata. Private inputs hold only the sensor readings or aggregates. The circuit guarantees the computational integrity of \(f\) on the private inputs under the supplied policy, so a verifier can rely on the result without seeing the readings. Typical instances are range or threshold checks, minimum/maximum over a set and the median over a window.

\section{Proving Pipelines}
The system supports two proving modes. The standard mode emits a zk‑SNARK per reading and verification effort grows with the number of readings. The recursive mode folds many steps and can produce a single compressed proof for a batch, which keeps verification essentially constant while shifting work to proving. Detailed tooling and commands are described in the Implementation chapter~\ref{chap:implementation}.

\section{Batch size policy}
Batching defines how many readings a single recursive step consumes. A small step size reduces work per step and can reduce memory pressure at the edge. It also increases the number of steps for a given dataset. The total proving cost is a combination of per step work and the final compression. A large step size reduces the number of steps and can improve end to end time once the prover amortizes the fixed costs of recursion and compression. Verification remains constant in both cases since the consumer checks a single proof for the entire batch. Padding to a multiple of the step size is part of the design and does not change the public claim. Very large step sizes can raise peak memory per step and increase latency until the final proof is available. If the input rate is irregular, larger step sizes can increase padding, which wastes capacity and shifts the effective input mix. A failure or restart also invalidates more work when steps are large. In practice a moderate step size is often ideal. It keeps per step memory bounded while reducing the total number of steps enough to move the time crossover toward smaller input sizes. The Results and Analysis chapter~\ref{chap:empirical-results} reports this effect explicitly.

\section{Security Properties and Assumptions}
The system keeps raw readings private and shares only proofs together with public policy parameters. The proofs ensure the computational integrity of the transformation. Authenticity follows from signature checks using the device certificate that is signed by the CA. The proving key remains on the orchestrator and the private device key remains on the device. We assume a trusted CA root, a correct ZoKrates setup and a verifier that is honest but curious.

\section{Performance Considerations}
We report proving time, verification time, proof size and the memory footprint of the process. The standard pipeline offers fine grained proofs but requires many verifications as the number of readings grows. The Nova pipeline amortizes verification to a nearly constant cost while adding overhead on the proving side.

\section{Limitations and Extensions}
Networking security such as TLS, key rotation, revocation via CRL or OCSP and Merkle based data binding are outside the scope of this chapter. A non zero knowledge baseline that computes the same transformations locally and ships only results can provide a reference for raw execution time and artefact size, but it does not meet the privacy requirement since raw inputs must be trusted or revealed during checking. Future work can bind entire reading sets through commitment roots, integrate certificate revocation and extend the circuit family with a sum in range billing scheme.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}
\label{chap:implementation}
\section{Environment and Hardware Profile}
\label{sec:env-hw}

We executed all experiments under a resource-constrained profile intended to simulate realistic IoT edge environments. 
Specifically, we used Docker containers with enforced limits on CPU and memory to approximate the hardware characteristics of IoT-devices. For example:

\begin{itemize}
  \item \textbf{CPU}: fixed 0.5 logical core
  \item \textbf{RAM}: fixed 1\,GB
\end{itemize}

These settings are chosen based on typical Raspberry Pi-class device specifications, which often provide 0.5-4 GB RAM and single-to-quad-core CPUs in edge deployments. For example, Gupta \& Nahrstedt \cite{guptaPerformanceCharacterizationContainers2025} empirically evaluate similar IoT devices and show that Docker containers on hardware with ~1 GB RAM suffer measurable overheads in latency and I/O under constrained CPU/RAM settings.\\

\noindent Host platform (for emulation):
\begin{table}[H]
  \centering
  \begin{tabular}{ll}
    \hline
    Component & Specification \\
    \hline
    CPU & AMD Ryzen 7 7800X3D (8 cores, 16 threads; base \SI{4.2}{GHz}) \\
    RAM & \SI{32}{GB} DDR5 @ \SI{6000}{MT/s} (2\,\texttimes{} DIMM) \\
    GPU & NVIDIA GeForce RTX 4070 SUPER (12\,GB VRAM) \\
    Virtualization & Windows 11 + WSL2 (Ubuntu kernel 6.6.87.2) \\
    \hline
  \end{tabular}
  \caption{Host platform used to simulate IoT-like environments under resource constraints via Docker/cgroups.}
\end{table}

\begin{figure}[H]
\centering
\resizebox{0.98\textwidth}{!}{%
\begin{tikzpicture}[
  scale=0.8, transform shape,
  lane/.style={draw, fill=gray!8, minimum width=3.7cm, minimum height=26cm},
  task/.style={draw, fill=white, rounded corners, minimum width=3.1cm, minimum height=0.95cm, font=\scriptsize, align=center},
  opt/.style={task, dashed},
  note/.style={draw, fill=gray!5, rounded corners, font=\scriptsize, align=left, minimum width=5.0cm},
  arrow/.style={-{Stealth[length=2.4mm,width=1.4mm]}, thick}
]
\node[lane] (L0) at (-4,0) {}; \node[above] at (L0.north) {Key Registry};
\node[lane] (L1) at ( 0,0) {}; \node[above] at (L1.north) {IoT Device};
\node[lane] (L2) at ( 4,0) {}; \node[above] at (L2.north) {Edge Orchestrator};
\node[lane] (L3) at ( 8,0) {}; \node[above] at (L3.north) {Prover};
\node[lane] (L4) at (12,0) {}; \node[above] at (L4.north) {Verifier};
\node[lane] (L5) at (16,0) {}; \node[above] at (L5.north) {Consumer};

\node[task] (d0) at (0,12.0)  {Generate device keypair\\(private stays on device)};
\node[task] (k1) at (-4,10.5) {Issue device credential (certification authority)};
\node[task] (k2) at (-4,9.0) {Publish authority root and device credential};

\node[task] (d1) at (0,7.5)  {Generate sensor data};
\node[task] (d2) at (0,6.0)  {Sign payload};

\node[task] (a0) at (4,4.5)  {Load authority root and device credential};
\node[task] (a1) at (4,3.0)  {Verify device certificate};
\node[task] (a2) at (4,1.5)  {Verify device signature (reading)};
\node[task] (a3) at (4,0.0)  {Setup ZoKrates (once)\\Manage proving.key \\ Export verification.key};
\node[task] (a4) at (4,-1.5)  {Apply computation + ZK proof\\(range, min/max, median, …)};
\node[task]  (a5) at (4,-3.0) {Prepare Nova inputs\\(init.json, steps.json)};

\node[task] (p1) at (8,-4.5) {Generate standard proof};
\node[task]  (p2) at (8,-6.0) {Recursive: prove + compress};

\node[task] (v0) at (12,-7.5) {Import verification.key};
\node[task] (v1) at (12,-9.0) {Verify standard proof};
\node[task]  (v2) at (12,-10.5) {Verify compressed proof};

\node[task] (c1) at (16,-12.0) {Receive verified result / aggregate};

\draw[arrow] (d0) to[out=180, in=90, looseness=1.2] node[above,font=\scriptsize]{1} (k1);
\draw[arrow] (k1) -- node[left,font=\scriptsize] {2} (k2);
\draw[arrow] (k2) to[out=-90, in=180, looseness=1.5] node[left, font=\scriptsize]{3} (a0);

\draw[arrow] (d1) -- node[right,font=\scriptsize] {4} (d2);
\draw[arrow] (d2) to[out=-90, in=180, looseness=1.5] node[left, font=\scriptsize]{5} (a2);

\draw[arrow] (a0) -- node[right,font=\scriptsize] {6} (a1);
\draw[arrow] (a1) -- node[right,font=\scriptsize] {7} (a2);
\draw[arrow] (a2) -- node[right,font=\scriptsize] {8} (a3);
\draw[arrow] (a3) -- node[right,font=\scriptsize] {9} (a4);

\draw[arrow] (a4) to[out=0, in=90, looseness=1.5] node[right, font=\scriptsize]{10} (p1);
\draw[arrow] (p1) to[out=0, in=90, looseness=1.5] node[right, font=\scriptsize]{11} (v0);
\draw[arrow] (v0) -- node[right,font=\scriptsize] {12} (v1);
\draw[arrow] (v1) to[out=0, in=90, looseness=1.5] node[right, font=\scriptsize]{13} (c1);

\draw[arrow] (a4) -- node[right,font=\scriptsize] {14} (a5);
\draw[arrow] (a5) to[out=-90, in=180, looseness=1.5] node[left, font=\scriptsize]{15} (p2);
\draw[arrow] (p2) to[out=-90, in=180, looseness=1.5] node[left, font=\scriptsize]{16} (v2);
\draw[arrow] (v2) to[out=-90, in=180, looseness=1.5] node[left, font=\scriptsize]{17} (c1);
\end{tikzpicture}
}
\caption{System Architecture with Key Registry, proving, and verification flows.}
\label{fig:system-overview}
\end{figure}

\section{Methodology}
The methodology fixes CPU and memory limits on the edge and measures wall clock time for proving and verification. Each configuration is executed with three repetitions and results report the mean across runs. Inputs are identical for the two proof pipelines. The non zero knowledge baseline computes the same range check without generating a proof to provide a lower bound for runtime and artefact size. The recursive pipeline uses a configurable step size and pads inputs to a multiple of that size. All measurements are taken inside the container to keep the environment stable and reproducible.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Requirements (load once in the preamble):
% \usepackage{graphicx}
% \usepackage{booktabs}
% \usepackage{siunitx}
% \usepackage{hyperref}
% \usepackage{caption}
% \sisetup{round-mode=places,round-precision=2,detect-weight=true,detect-inline-weight=math}
% =========================
% RESULTS CHAPTER (custom captions per run)
% =========================

\chapter{Results and Analysis}
\label{chap:empirical-results}

% ====== PREAMBLE ADDITIONS (einmalig in die Präambel) ======
% \usepackage{graphicx}
% \usepackage{float}
% \usepackage{booktabs}
% \usepackage{adjustbox}
% \usepackage{csvsimple}
% % Optionale feine Tabelle:
% % \usepackage{pgfplotstable}

This chapter compares a Standard zk-SNARK pipeline with a recursive Nova zk-SNARK pipeline in resource-limited containers. Each run fixes CPU/RAM limits and sweeps the number of IoT readings $N$. Every overview plot shows: (i) total wall-clock time, (ii) total proof size, and (iii) the time-efficiency ratio \emph{Standard/Nova} (values $>1$ favor Nova). For each $N$ both pipelines process identical inputs. Nova builds an incrementally verifiable computation (IVC) and compresses to one succinct proof; Standard emits one Groth16 proof per instance. Transport security and device authentication are out of scope to isolate proving/verification cost.

\section{Results}

% ---- ONE MACRO PER RUN: custom title + custom limits + observation text + CSV table ----
\newcommand{\ResultRunCustom}[4]{%
  \subsection*{#2}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ergebnis/md_warm_nr#1/real_crossover_overview.png}
    \caption{#2, limits: \texttt{#3}.}
    \label{fig:run-#1}
  \end{figure}

  #4

  % ---- CSV-Tabelle mit den Messwerten dieses Runs ----
  \begin{table}[H]
    \centering
    % Kurzfassung für List of Tables | Volltext unter der Tabelle
    \caption[Results — Run \##1]{Results — Run \##1 (limits: \texttt{#3})}
    \label{tab:run-#1-summary}
    \begin{adjustbox}{max width=\linewidth}
      \csvautotabular[
        separator=comma,
        head to column names,
        respect all
      ]{data/visualizations/md_warm_nr#1/crossover_summary.csv}
    \end{adjustbox}
  \end{table}

  \vspace{0.75em}
}

% ===== RUNS (Limits je Run hier eintragen) =====
\ResultRunCustom{1}{Run \#1}{--cpus=0.5, --memory=1g}{
\textbf{Observation.} Standard is faster across $N\in[100,1000]$ and the efficiency ratio remains below one. The recursive proof size remains essentially constant while the standard proof size grows roughly linearly. The size crossover appears near eighty two readings. No time crossover is present in this configuration.
}

\ResultRunCustom{2}{Run \#2}{--cpus=1, --memory=2g}{
\textbf{Observation.} Same qualitative pattern up to $N=500$: Standard wins in time (ratio $\approx0.15\text{–}0.39$); Nova’s proof size is essentially constant ($\approx70$\,KB), Standard grows linearly. Size crossover $\approx 82$; no time crossover.
}

\ResultRunCustom{3}{Run \#3}{--cpus=1, --memory=4}{
\textbf{Observation.} Matches runs \#1 and \#2. The time ratio remains below one throughout and the recursive pipeline is slower on the edge. The size crossover appears around eighty two readings.
}

\ResultRunCustom{4}{Run \#4}{--cpus=1, --memory=8}{
\textbf{Observation.} Standard remains faster end to end and the recursive pipeline produces a single stable proof. The size crossover is close to eighty two readings and no time crossover occurs up to five hundred readings.
}

\ResultRunCustom{5}{Run \#5}{--cpus=2, --memory=2}{
\textbf{Observation.} Time ratio rises with $N$ (Standard approaches Nova) but remains $<1$ for $N\le 500$. Proof-size behavior unchanged (Nova $\approx70$\,KB vs.\ linear Standard). Size crossover near $82$.
}

\ResultRunCustom{6}{Run \#6}{--cpus=4, --memory=2}{
\textbf{Observation.} A time crossover is present and the efficiency crosses one at roughly seven hundred readings in this setting. The recursive pipeline is faster beyond that point. The size crossover remains around eighty two readings.
}

\ResultRunCustom{7}{Run \#7}{--cpus=4, --memory=2g, batch size 10}{
\textbf{Observation.} The time crossover occurs at four hundred readings where the recursive path becomes faster and the advantage increases with input size. Nova’s proof size remains near sixty nine kilobytes while the standard proof grows linearly.
}

\ResultRunCustom{8}{Run \#8}{--cpus=4, --memory=2g, batch size 20}{
\textbf{Observation.} The time crossover appears already at three hundred readings and persists beyond. Larger step size amortizes fixed costs at the price of higher peak memory per step.
}

\ResultRunCustom{9}{Run \#9}{--cpus=0.5, --memory=1g, batch size 20}{
\textbf{Observation.} Under tighter CPU and memory limits the time crossover moves to five hundred readings. The general shape remains consistent and the size behavior is unchanged.
}

\ResultRunCustom{10}{Run \#10}{--cpus=0.5, --memory=1g, batch size 50}{
\textbf{Observation.} With very large steps the time crossover appears at four hundred readings despite tighter CPU and memory. Peak memory per step rises and the recursive prover benefits from fewer folds.
}

\section{Compact table}
\begin{table}[H]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Run & Size crossover & Time crossover & Qualitative verdict \\
    \midrule
    \#1 & $\approx 82$ & none (to $N{=}1000$) & Standard faster; Nova much smaller \\
    \#2 & $\approx 82$ & none (to $N{=}500$)  & Same trend as \#1 \\
    \#3 & $\approx 82$ & none (to $N{=}500$)  & Same trend as \#1 \\
    \#4 & $\approx 82$ & none (to $N{=}500$)  & Same trend as \#1 \\
    \#5 & $\approx 82$ & none (to $N{=}500$)  & Gap narrows; size still favors Nova \\
    \#6 & $\approx 82$ & $\approx 700$ & Nova overtakes beyond $N\approx700$ \\
    \#7 & $\approx 82$ & $\approx 400$ & batch size 10, Nova faster beyond 400 \\
    \#8 & $\approx 82$ & $\approx 300$ & batch size 20, crossover at 300 \\
    \#9 & $\approx 82$ & $\approx 500$ & batch size 20, constrained CPU/RAM \\
    \#10 & $\approx 82$ & $\approx 400$ & batch size 50, constrained CPU/RAM \\
    \bottomrule
  \end{tabular}
  \caption{Crossover summary from runs \#1–\#10.}
  \label{tab:crossover-summary}
\end{table}

\section{Cross-run synthesis}
Across runs \#1–\#10 three stable findings emerge:
\begin{enumerate}
  \item \textbf{Proof size.} Nova’s compressed proof remains essentially constant ($\approx70$\,KB) as $N$ grows, whereas Standard increases roughly linearly at $\sim0.85$\,KB per reading. Hence a size crossover appears around \[
\frac{70\,\text{KB}}{0.85\,\text{KB}} \approx 82~\text{Readings}\] in all runs.

  \item \textbf{Total time.} On constrained edge hardware, Standard is consistently faster for small to medium $N$. With larger $N$ or more favorable limits, the gap narrows. With a step size of ten the time efficiency crosses one at $N=400$ and Nova remains faster beyond that point.
  \item \textbf{Operational trade-off.} Nova minimizes verifier load (one constant-size proof) and can become time-competitive at larger $N$, but incurs higher proving overhead at small $N$. Standard minimizes edge proving time for small/mid batches but yields a linearly growing set of proofs.
\end{enumerate}

\section{Batch size sensitivity}
The recursive pipeline depends on the chosen step size. With a step size of ten the time crossover appears at four hundred readings. Nova is faster from that point and the advantage grows with the input size. Smaller step sizes produce more steps and push the time crossover to the right since the prover spends more work on repeated step transitions. Larger step sizes reduce the number of steps and move the crossover to the left once the prover amortizes the fixed costs of recursion and compression. The choice is a trade off between memory headroom and end to end time. The verifier cost remains constant since a single proof is checked in all cases. The results show that tuning the step size is an effective control to reach a desired operating point on a given device profile.

\section{Modeling choices and alternative views}
The overview plots aggregate proving and verification into single totals per pipeline and per input size. This summarizes the user visible cost and facilitates crossover detection. An alternative presentation plots proving and verification as separate series over the number of inputs. Such a view highlights that verifier time is flat for the recursive pipeline while it grows with the number of items in the standard pipeline. Both views are consistent. The aggregated view is used throughout to keep the comparison concise, and the separate view is supplied when tighter diagnostics are needed.

\section{Summary for the smart-home edge}
For a resource-limited hub aggregating frequent, low-payload readings, the better choice depends on the bottleneck: when edge CPU/RAM and latency dominate, Standard is preferable at small $N$; when consumer bandwidth/verification dominates—or when batches become large—Nova’s single succinct proof is advantageous and can even overtake Standard in total time once $N$ is sufficiently high (run~\#7).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion}
\label{chap:discussion}

\section{Privacy implications of recursion}
Recursive proving reduces the number of proofs a consumer must check to a single constant sized claim that covers many steps. In privacy terms this limits the exposure surface because consumers receive one proof per batch rather than many separate artifacts. The transformation logic and public parameters remain identical to the standard pipeline. The confidentiality guarantee is the same since both pipelines are zero knowledge with respect to the private readings. The practical difference concerns interfaces and logging. A single recursive proof invites simpler handling and avoids per reading side channels in application logs or transport that might correlate with private structure. At the same time recursion changes the amortization of costs at the edge. If batches remain small or if latency is strict then the standard pipeline can be the preferable choice. The privacy benefit of recursion is therefore indirect. It simplifies verification and system interfaces at the consumer, which can reduce accidental information flows in practice, while preserving the same formal guarantees.

\section{Alternative Privacy-Enhancing Technologies}

While our evaluation focuses on zk-SNARKs for verifiable IoT aggregation, several alternative privacy-enhancing technologies (PETs) offer different trade-offs in similar contexts. Understanding these alternatives helps position our contribution within the broader landscape of privacy-preserving IoT systems.

\subsection{Complementary PETs and Their Trade-offs}

\textbf{Differential Privacy} protects individuals by adding calibrated noise to aggregated results, but sacrifices utility and does not guarantee computational integrity \cite{dworkCalibratingNoiseSensitivity2006,dworkAlgorithmicFoundationsDifferential2013}. While effective for statistical privacy, differential privacy cannot provide the exact verification guarantees required for financial or regulatory compliance scenarios.

\textbf{Secure Multi-Party Computation (MPC)} allows computation over distributed inputs without a trusted third party, yet typically incurs high latency and communication overheads that are challenging for constrained IoT devices \cite{yaoHowGenerateExchange1986,goldreichHowPlayANY1987,damgardMultipartyComputationSomewhat2012}. The continuous communication requirements make MPC less suitable for the intermittent connectivity patterns common in IoT deployments.

\textbf{Trusted Execution Environments (TEEs)} provide hardware-backed isolation for secure computation, but introduce additional trust assumptions and are vulnerable to side-channel attacks. Furthermore, TEE availability varies significantly across IoT device classes, limiting deployment flexibility.

\textbf{ZK-STARKs} offer transparency and post-quantum security, but generally require larger proof sizes and higher prover costs compared to SNARK systems, making them less suitable for bandwidth-constrained IoT environments.

\subsection{Why zk-SNARKs for IoT Aggregation}

Given our goal of verifiable aggregation under device and bandwidth constraints, zk-SNARKs provide the optimal balance of proof succinctness, verification efficiency, and privacy guarantees. Our empirical evaluation demonstrates that recursive zk-SNARKs (Nova) can achieve constant-size verification while maintaining computational integrity, making them particularly suitable for resource-constrained IoT environments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion \& Future Work}
\label{chap:conclusion}
This thesis examined whether recursive zk-SNARKs are practically beneficial for verifiable IoT aggregation on constrained edge hardware. We implemented two pipelines that share the same transformation semantics and trust assumptions: a standard Groth16 path and a recursive Nova path that optionally compresses the accumulated recursion state to a single standard-compatible proof. Both pipelines provide the same guarantees. The standard path scales time and artifact size with the number of inputs, while the Nova path concentrates effort in proving and leaves one constant‑size verification for the consumer.

Our measurements on a dockerized edge environment with explicit CPU and memory limits show a consistent pattern. Proof size is essentially constant for the recursive pipeline while it grows roughly linearly for the standard pipeline, which yields a size crossover around 82 readings in all configurations we tested. Total time on the edge favors the standard pipeline for small to medium input sizes. As batches grow and with a step size of ten, the time efficiency crosses one at four hundred readings and the recursive pipeline becomes faster beyond that point. Larger step sizes shift the crossover to smaller input sizes by amortizing fixed folding costs, at the expense of higher peak memory and fewer opportunities to stream or preempt. Tighter CPU and memory limits shift the crossover to the right as expected, but do not change the qualitative picture. These results confirm that recursion can deliver constant verification cost and competitive end-to-end latency once inputs are sufficiently large or when consumer-side constraints dominate.

The architectural implications are straightforward. When low latency for small batches is critical or hardware is very weak, the standard pipeline remains the pragmatic choice. When verification and bandwidth at the consumer are the bottleneck, or when aggregation naturally yields larger batches, the recursive pipeline provides operational simplicity through a single succinct proof and can become time competitive or superior. The choice of step size is a tunable control: increasing it reduces per-batch overhead and advances the time crossover, but raises peak prover memory and increases the cost of aborts and padding. In our environment step size ten struck a robust balance.

From a privacy perspective both pipelines provide the same formal guarantees since they differ only in proof composition. In practice the recursive pipeline simplifies interfaces by exposing a single proof per batch, which reduces ancillary metadata and potential side channels in application logs or transport.

This work has limitations. The evaluation uses synthetic yet structured sensor traces, a single host platform with docker-imposed CPU and memory limits, and a specific tooling stack and curve (ZoKrates 0.8.8 on Pallas). Caching and I/O can bias timings and we mitigate this by keeping the environment stable and by averaging multiple replicates. These factors bound external validity; however, the qualitative findings about size constancy, crossover shifts with step size, and verifier cost concentration are robust across our runs.

Practitioners can apply three concrete guidelines. Use the standard pipeline for small batches or strict tail latency on weak edge devices. Use the recursive pipeline for batches beyond the measured crossover or when consumer bandwidth and verification time dominate, and prefer the compressed variant for portability. Tune step size to meet memory headroom while seeking earlier time crossovers; in our setting step size ten performed well, step size twenty advanced the crossover further at the cost of higher peak memory, and step size five delayed it.

Future work should bind input sets cryptographically and externally via Merkle commitments to support selective disclosure and end-to-end set attestation. A fully external verifier service for consumers would decouple verification from the edge environment. Exploring alternative folding schemes and transparent proof systems may further reduce proving latency without sacrificing succinct verification. Finally, extending the evaluation to heterogeneous devices and real sensor deployments would strengthen the operational guidance and refine step size policies under diverse workloads.

\printbibliography

\end{document}


