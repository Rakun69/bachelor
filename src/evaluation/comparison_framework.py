"""
Comparison Framework for Classical vs Recursive zk-SNARKs
Implements fair comparison with identical batches and real metrics only
"""

import json
import time
import psutil
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np

from proof_systems.snark_manager import SNARKManager
from proof_systems.zokrates_nova_manager import ZoKratesNovaManager
from iot_simulation.smart_home import SmartHomeSensors

logger = logging.getLogger(__name__)

@dataclass
class ComparisonMetrics:
    """Real metrics for SNARK comparison - NO SIMULATIONS"""
    approach: str  # "snark" or "recursive"
    batch_size: int
    n_inputs: int
    t_prove_ms: float
    t_verify_ms: float  
    mem_peak_mb: Optional[float] = None
    proof_size_bytes: int = 0
    cpu_util_avg: Optional[float] = None
    energy_j: Optional[float] = None
    
    # Derived metrics
    @property
    def time_per_item_ms(self) -> float:
        return self.t_prove_ms / max(self.n_inputs, 1)
    
    @property 
    def throughput_items_per_s(self) -> float:
        return (self.n_inputs * 1000) / max(self.t_prove_ms, 1)

@dataclass
class EfficiencyWeights:
    """
    Efficiency Index weights with scientific justification:
    
    Time (0.5): Primary bottleneck in IoT scenarios - real-time constraints
    Memory (0.2): Critical for resource-constrained IoT devices  
    Size (0.2): Network bandwidth and storage limitations in IoT
    Energy (0.1): Correlated with time/CPU, but visible for sustainability analysis
    
    Reference: Weighted-Sum aggregation is standard in Multi-Criteria Decision Analysis
    """
    time: float = 0.5
    memory: float = 0.2
    size: float = 0.2
    energy: float = 0.1
    
    def validate(self) -> bool:
        total = self.time + self.memory + self.size + self.energy
        return abs(total - 1.0) < 0.001

class ComparisonFramework:
    """
    Fair comparison framework: Classical vs Recursive SNARKs
    STRICT REQUIREMENT: Only real measurements, fail-fast on any errors
    """
    
    def __init__(self, output_dir: str = "data/comparison"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize managers
        self.snark_manager = SNARKManager()
        self.nova_manager = ZoKratesNovaManager()
        self.iot_simulator = SmartHomeSensors()
        
        # Efficiency weights with justification
        self.weights = EfficiencyWeights()
        assert self.weights.validate(), "Efficiency weights must sum to 1.0"
        
        logger.info("ComparisonFramework initialized - REAL METRICS ONLY")
    
    def run_comparison(self, batch_sizes: List[int], max_data_size: int = 500) -> Dict[str, Any]:
        """
        Run fair comparison on identical batches for both approaches
        FAIL-FAST: If either approach fails, abort immediately with clear error
        """
        logger.info(f"Starting FAIR comparison: Classical vs Recursive SNARKs")
        logger.info(f"Batch sizes: {batch_sizes}, Max data: {max_data_size}")
        
        all_metrics = []
        crossover_analysis = {}
        
        try:
            for batch_size in batch_sizes:
                logger.info(f"Testing batch size: {batch_size}")
                
                # Generate identical test data for both approaches
                test_data = self._generate_test_data(batch_size * 3)  # Ensure enough data
                
                if len(test_data) < batch_size:
                    raise Exception(f"FAIL: Insufficient test data for batch size {batch_size}")
                
                # Test Classical SNARKs
                classical_metrics = self._test_classical_snark(test_data, batch_size)
                if not classical_metrics:
                    raise Exception(f"classical_proof_failed: Batch size {batch_size}")
                
                # Test Recursive SNARKs (Nova) 
                recursive_metrics = self._test_recursive_snark(test_data, batch_size)
                if not recursive_metrics:
                    raise Exception(f"recursive_proof_failed: Batch size {batch_size}")
                
                all_metrics.extend([classical_metrics, recursive_metrics])
                logger.info(f"Batch {batch_size}: Both approaches successful")
            
            # Calculate efficiency indices and crossover points
            all_metrics = self._calculate_efficiency_indices(all_metrics)
            crossover_analysis = self._analyze_crossover_points(all_metrics)
            
            # Generate unified comparison table
            table_data = self._generate_comparison_table(all_metrics)
            
            # Save results
            results = {
                "comparison_successful": True,
                "metrics": all_metrics,
                "crossover_analysis": crossover_analysis,
                "table_data": table_data,
                "efficiency_weights": {
                    "time": self.weights.time,
                    "memory": self.weights.memory, 
                    "size": self.weights.size,
                    "energy": self.weights.energy,
                    "justification": "Time=0.5 (IoT real-time), Memory/Size=0.2 each (device constraints), Energy=0.1 (sustainability)"
                }
            }
            
            output_file = self.output_dir / "snark_comparison_results.json"
            with open(output_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            logger.info(f"Comparison completed successfully. Results: {output_file}")
            return results
            
        except Exception as e:
            error_msg = f"COMPARISON_FAILED: {str(e)}"
            logger.error(error_msg)
            return {
                "comparison_successful": False,
                "error": error_msg,
                "partial_metrics": all_metrics if all_metrics else []
            }
    
    def _generate_test_data(self, size: int) -> List[Dict[str, Any]]:
        """Generate identical test data for both approaches"""
        logger.info(f"Generating {size} test IoT readings")
        
        # Generate realistic IoT sensor data
        readings = []
        for i in range(size):
            reading = {
                'timestamp': time.time() + i,
                'sensor_id': f'sensor_{i % 5}',
                'value': 20.0 + (i % 20),  # Temperature-like values
                'type': 'temperature'
            }
            readings.append(reading)
        
        return readings
    
    def _test_classical_snark(self, test_data: List[Dict], batch_size: int) -> Optional[ComparisonMetrics]:
        """Test classical ZoKrates SNARKs with real metrics"""
        try:
            logger.info(f"Testing Classical SNARK: batch_size={batch_size}")
            
            # Use first batch_size items
            batch_data = test_data[:batch_size]
            
            # Prepare circuit inputs (using filter_range as representative)
            circuit_inputs = self._prepare_snark_inputs(batch_data)
            
            # Measure performance
            process = psutil.Process()
            mem_before = process.memory_info().rss / 1024 / 1024  # MB
            cpu_start = process.cpu_percent()
            
            prove_start = time.time()
            result = self.snark_manager.generate_proof("filter_range", circuit_inputs)
            prove_time = (time.time() - prove_start) * 1000  # ms
            
            if not result.success:
                logger.error(f"Classical SNARK failed: {result.error_message}")
                return None
            
            # Verify proof
            verify_start = time.time()
            verify_result = self.snark_manager.verify_proof("filter_range", result.proof, result.public_inputs)
            verify_time = (time.time() - verify_start) * 1000  # ms
            
            if not verify_result:
                logger.error("Classical SNARK verification failed")
                return None
            
            # Measure final memory
            mem_after = process.memory_info().rss / 1024 / 1024  # MB
            cpu_end = process.cpu_percent()
            
            return ComparisonMetrics(
                approach="snark",
                batch_size=batch_size,
                n_inputs=len(batch_data),
                t_prove_ms=prove_time,
                t_verify_ms=verify_time,
                mem_peak_mb=mem_after - mem_before if mem_after > mem_before else None,
                proof_size_bytes=len(result.proof.encode()) if result.proof else 0,
                cpu_util_avg=(cpu_start + cpu_end) / 2 if cpu_end > 0 else None
            )
            
        except Exception as e:
            logger.error(f"Classical SNARK test failed: {e}")
            return None
    
    def _test_recursive_snark(self, test_data: List[Dict], batch_size: int) -> Optional[ComparisonMetrics]:
        """Test recursive Nova SNARKs with real metrics"""
        try:
            logger.info(f"Testing Recursive SNARK (Nova): batch_size={batch_size}")
            
            # Split into Nova batches (10 items per step for Nova circuit)
            nova_batches = []
            for i in range(0, len(test_data), 3):
                batch = test_data[i:i+3]
                if len(batch) > 0:
                    nova_batches.append(batch)
            
            if not nova_batches:
                logger.error("No valid Nova batches created")
                return None
            
            # Measure performance  
            process = psutil.Process()
            mem_before = process.memory_info().rss / 1024 / 1024  # MB
            cpu_start = process.cpu_percent()
            
            prove_start = time.time()
            nova_result = self.nova_manager.prove_recursive_batch(nova_batches)
            prove_time = (time.time() - prove_start) * 1000  # ms
            
            if not nova_result.success:
                logger.error(f"Recursive SNARK failed: {nova_result.error_message}")
                return None
            
            # Measure final memory
            mem_after = process.memory_info().rss / 1024 / 1024  # MB
            cpu_end = process.cpu_percent()
            
            return ComparisonMetrics(
                approach="recursive",
                batch_size=batch_size,
                n_inputs=len(test_data),
                t_prove_ms=prove_time,
                t_verify_ms=nova_result.verify_time * 1000,  # Convert to ms
                mem_peak_mb=mem_after - mem_before if mem_after > mem_before else None,
                proof_size_bytes=nova_result.proof_size,
                cpu_util_avg=(cpu_start + cpu_end) / 2 if cpu_end > 0 else None
            )
            
        except Exception as e:
            logger.error(f"Recursive SNARK test failed: {e}")
            return None
    
    def _prepare_snark_inputs(self, batch_data: List[Dict]) -> Dict[str, Any]:
        """Prepare inputs for classical SNARK circuit (filter_range)"""
        # Take first value for filter_range circuit
        value = int(batch_data[0].get('value', 25) * 10) if batch_data else 250  # Scale to int
        
        return {
            "value": str(value),
            "min": "200",  # 20.0 scaled
            "max": "300"   # 30.0 scaled
        }
    
    def _calculate_efficiency_indices(self, metrics: List[ComparisonMetrics]) -> List[ComparisonMetrics]:
        """
        Calculate efficiency index using weighted normalization
        Formula: efficiency_index = 0.5*norm_time + 0.2*norm_mem + 0.2*norm_size + 0.1*norm_energy
        """
        logger.info("Calculating efficiency indices with real metrics only")
        
        # Group by batch size for normalization
        batch_groups = {}
        for metric in metrics:
            if metric.batch_size not in batch_groups:
                batch_groups[metric.batch_size] = []
            batch_groups[metric.batch_size].append(metric)
        
        # Calculate normalized indices per batch size
        for batch_size, batch_metrics in batch_groups.items():
            # Find best values for normalization (minimum = best)
            times = [m.t_prove_ms for m in batch_metrics]
            sizes = [m.proof_size_bytes for m in batch_metrics if m.proof_size_bytes > 0]
            memories = [m.mem_peak_mb for m in batch_metrics if m.mem_peak_mb is not None]
            energies = [m.energy_j for m in batch_metrics if m.energy_j is not None]
            
            best_time = min(times) if times else 1.0
            best_size = min(sizes) if sizes else 1.0
            best_memory = min(memories) if memories else 1.0
            best_energy = min(energies) if energies else 1.0
            
            # Calculate efficiency index for each metric
            for metric in batch_metrics:
                norm_time = metric.t_prove_ms / best_time
                norm_size = metric.proof_size_bytes / best_size if metric.proof_size_bytes > 0 else 1.0
                norm_memory = metric.mem_peak_mb / best_memory if metric.mem_peak_mb else 1.0
                norm_energy = metric.energy_j / best_energy if metric.energy_j else 1.0
                
                # Weighted efficiency index 
                efficiency_index = (
                    self.weights.time * norm_time +
                    self.weights.memory * norm_memory +
                    self.weights.size * norm_size +
                    self.weights.energy * norm_energy
                )
                
                # Add as dynamic attribute
                metric.efficiency_index = efficiency_index
                
                logger.debug(f"{metric.approach} batch_{batch_size}: efficiency={efficiency_index:.3f}")
        
        return metrics
    
    def _analyze_crossover_points(self, metrics: List[ComparisonMetrics]) -> Dict[str, Any]:
        """
        Find crossover points where recursive becomes superior
        Only calculate if both approaches have valid metrics
        """
        logger.info("Analyzing crossover points")
        
        batch_sizes = sorted(set(m.batch_size for m in metrics))
        crossovers = {
            "efficiency_crossover": None,
            "time_crossover": None,
            "analysis": {}
        }
        
        for batch_size in batch_sizes:
            batch_metrics = [m for m in metrics if m.batch_size == batch_size]
            snark_metrics = [m for m in batch_metrics if m.approach == "snark"]
            recursive_metrics = [m for m in batch_metrics if m.approach == "recursive"]
            
            if not snark_metrics or not recursive_metrics:
                continue
                
            snark_m = snark_metrics[0]
            recursive_m = recursive_metrics[0]
            
            # Check efficiency crossover
            if (hasattr(recursive_m, 'efficiency_index') and hasattr(snark_m, 'efficiency_index')):
                if recursive_m.efficiency_index <= snark_m.efficiency_index:
                    if crossovers["efficiency_crossover"] is None:
                        crossovers["efficiency_crossover"] = batch_size
            
            # Check time crossover
            if recursive_m.t_prove_ms <= snark_m.t_prove_ms:
                if crossovers["time_crossover"] is None:
                    crossovers["time_crossover"] = batch_size
            
            # Store detailed analysis
            crossovers["analysis"][f"batch_{batch_size}"] = {
                "snark_time_ms": snark_m.t_prove_ms,
                "recursive_time_ms": recursive_m.t_prove_ms,
                "snark_efficiency": getattr(snark_m, 'efficiency_index', None),
                "recursive_efficiency": getattr(recursive_m, 'efficiency_index', None),
                "recursive_advantage": recursive_m.t_prove_ms < snark_m.t_prove_ms
            }
        
        return crossovers
    
    def _generate_comparison_table(self, metrics: List[ComparisonMetrics]) -> List[Dict[str, Any]]:
        """Generate unified comparison table as requested"""
        logger.info("Generating unified comparison table")
        
        table_rows = []
        for metric in metrics:
            row = {
                "approach": metric.approach,
                "batch_size": metric.batch_size,
                "n_inputs": metric.n_inputs,
                "t_prove_ms": round(metric.t_prove_ms, 2),
                "t_verify_ms": round(metric.t_verify_ms, 2),
                "mem_peak_mb": round(metric.mem_peak_mb, 2) if metric.mem_peak_mb else "N/A",
                "proof_size_bytes": metric.proof_size_bytes,
                "cpu_util_avg": round(metric.cpu_util_avg, 1) if metric.cpu_util_avg else "N/A",
                "energy_j": round(metric.energy_j, 3) if metric.energy_j else "N/A",
                "time_per_item_ms": round(metric.time_per_item_ms, 3),
                "throughput_items_per_s": round(metric.throughput_items_per_s, 1),
                "efficiency_index": round(getattr(metric, 'efficiency_index', 0), 3)
            }
            table_rows.append(row)
        
        return sorted(table_rows, key=lambda x: (x["batch_size"], x["approach"]))

def main():
    """Test the comparison framework"""
    framework = ComparisonFramework()
    
    # Test with different batch sizes
    batch_sizes = [5, 10, 20, 50]
    results = framework.run_comparison(batch_sizes)
    
    if results.get("comparison_successful"):
        print("‚úÖ Comparison completed successfully!")
        print(f"üìä Results saved to: {framework.output_dir}/snark_comparison_results.json")
        
        # Print crossover analysis
        crossover = results.get("crossover_analysis", {})
        print(f"\nüéØ CROSSOVER ANALYSIS:")
        print(f"   Efficiency Crossover: {crossover.get('efficiency_crossover', 'Not found')} batch size")
        print(f"   Time Crossover: {crossover.get('time_crossover', 'Not found')} batch size")
    else:
        print(f"‚ùå Comparison failed: {results.get('error')}")

if __name__ == "__main__":
    main()
