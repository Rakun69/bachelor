#!/usr/bin/env python3
"""
Large Scale Evaluation Tests
Testet das System mit groÃŸen DatensÃ¤tzen und findet den echten Crossover-Point
"""

import sys
import os
import time
import json
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

def test_crossover_point_analysis():
    """Finde den echten Crossover-Point zwischen Standard und Recursive SNARKs"""
    print("=" * 80)
    print("LARGE SCALE: Crossover-Point Analyse")
    print("=" * 80)
    
    try:
        from src.proof_systems.snark_manager import SNARKManager
        from src.proof_systems.fixed_nova_manager import FixedZoKratesNovaManager
        
        # Setup Manager
        standard_manager = SNARKManager(circuits_dir="circuits", output_dir="data/test_proofs")
        nova_manager = FixedZoKratesNovaManager()
        
        circuit_path = Path("circuits/basic/filter_range.zok")
        standard_manager.compile_circuit(str(circuit_path), "filter_range")
        standard_manager.setup_circuit("filter_range")
        nova_manager.setup()
        
        # Test grÃ¶ÃŸere DatensÃ¤tze: 30, 60, 90, 120, 150 Items
        test_sizes = [30, 60, 90, 120, 150]
        
        results = []
        crossover_found = False
        
        for data_size in test_sizes:
            print(f"\nğŸ“Š TESTE {data_size} IoT READINGS")
            print("-" * 50)
            
            # Generiere Test Daten
            test_readings = []
            for i in range(data_size):
                test_readings.append({
                    "value": 20 + (i % 15),  # 20-34Â°C Variation
                    "sensor_type": "temperature"
                })
            
            # Standard SNARKs Test
            print(f"ğŸ” Standard SNARKs: {data_size} Proofs...")
            
            standard_start = time.time()
            standard_success_count = 0
            standard_total_size = 0
            
            # Teste nur erste 10 fÃ¼r Performance (extrapoliere den Rest)
            sample_size = min(10, data_size)
            for i in range(sample_size):
                reading = test_readings[i]
                inputs = ["15", "40", str(int(reading["value"]))]
                result = standard_manager.generate_proof("filter_range", inputs)
                
                if result.success:
                    standard_success_count += 1
                    standard_total_size += result.metrics.proof_size
            
            sample_time = time.time() - standard_start
            
            # Extrapoliere fÃ¼r alle Items
            if standard_success_count > 0:
                avg_time_per_proof = sample_time / sample_size
                avg_size_per_proof = standard_total_size / standard_success_count
                
                standard_total_time = avg_time_per_proof * data_size
                standard_total_size = avg_size_per_proof * data_size
                
                print(f"   âœ… Sample: {standard_success_count}/{sample_size} erfolgreich")
                print(f"   â±ï¸  Extrapolierte Zeit: {standard_total_time:.3f}s")
                print(f"   ğŸ“¦ Extrapolierte GrÃ¶ÃŸe: {standard_total_size:,.0f} bytes")
            else:
                print(f"   âŒ Standard SNARKs fehlgeschlagen")
                continue
            
            # Nova Recursive Test
            print(f"ğŸ”„ Nova Recursive: 1 Proof fÃ¼r {data_size} Items...")
            
            # Teile in 3er Batches
            batches = []
            for i in range(0, len(test_readings), 3):
                batch = test_readings[i:i+3]
                while len(batch) < 3:
                    batch.append({"value": 0, "sensor_type": "padding"})
                batches.append(batch)
            
            nova_start = time.time()
            nova_result = nova_manager.prove_recursive_batch(batches)
            
            if nova_result.success:
                print(f"   âœ… Nova Proof erfolgreich")
                print(f"   â±ï¸  Zeit: {nova_result.total_time:.3f}s")
                print(f"   ğŸ“¦ Proof GrÃ¶ÃŸe: {nova_result.proof_size:,} bytes")
                print(f"   ğŸ”„ Steps: {nova_result.step_count}")
                
                # Crossover Analyse
                time_ratio = standard_total_time / nova_result.total_time
                size_ratio = standard_total_size / nova_result.proof_size
                
                print(f"\nğŸ“ˆ CROSSOVER ANALYSE:")
                print(f"   âš¡ Zeit-VerhÃ¤ltnis: {time_ratio:.2f}x")
                print(f"   ğŸ’¾ GrÃ¶ÃŸe-VerhÃ¤ltnis: {size_ratio:.2f}x")
                
                if time_ratio > 1.0 and not crossover_found:
                    print(f"   ğŸ¯ CROSSOVER GEFUNDEN! Nova wird ab {data_size} Items effizienter!")
                    crossover_found = True
                elif time_ratio > 1.0:
                    print(f"   âœ… Nova Vorteil bestÃ¤tigt bei {data_size} Items")
                else:
                    print(f"   âš ï¸  Standard noch effizienter bei {data_size} Items")
                
                results.append({
                    "data_size": data_size,
                    "standard_time": standard_total_time,
                    "standard_size": standard_total_size,
                    "nova_time": nova_result.total_time,
                    "nova_size": nova_result.proof_size,
                    "time_ratio": time_ratio,
                    "size_ratio": size_ratio,
                    "nova_advantage": time_ratio > 1.0
                })
            else:
                print(f"   âŒ Nova fehlgeschlagen: {nova_result.error_message}")
        
        # Zusammenfassung
        print("\n" + "=" * 80)
        print("ğŸ¯ CROSSOVER-POINT ANALYSE ERGEBNISSE")
        print("=" * 80)
        
        print(f"{'Items':<8} {'Std Zeit':<12} {'Nova Zeit':<12} {'Zeit Ratio':<12} {'Nova Besser':<12}")
        print("-" * 70)
        
        for r in results:
            advantage = "âœ… JA" if r["nova_advantage"] else "âŒ NEIN"
            print(f"{r['data_size']:<8} {r['standard_time']:<12.2f} {r['nova_time']:<12.2f} {r['time_ratio']:<12.2f} {advantage:<12}")
        
        # Speichere Ergebnisse
        results_file = Path("data/test_benchmarks/crossover_analysis.json")
        results_file.parent.mkdir(parents=True, exist_ok=True)
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ“Š Ergebnisse gespeichert: {results_file}")
        
        return True, results
        
    except Exception as e:
        print(f"âŒ Crossover Analyse fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_multi_circuit_performance():
    """Teste verschiedene Circuits mit grÃ¶ÃŸeren DatensÃ¤tzen"""
    print("\n" + "=" * 80)
    print("LARGE SCALE: Multi-Circuit Performance Test")
    print("=" * 80)
    
    try:
        from src.proof_systems.snark_manager import SNARKManager
        
        manager = SNARKManager(circuits_dir="circuits", output_dir="data/test_proofs")
        
        # Circuits zum Testen
        circuits_to_test = [
            ("circuits/basic/filter_range.zok", "filter_range", ["10", "50", "25"]),
            ("circuits/basic/min_max.zok", "min_max", 
             ["10", "20", "30", "40", "50", "15", "25", "35", "45", "55", "5", "15"]),  # 10 values + expected min/max
            ("circuits/basic/median.zok", "median", 
             ["10", "20", "30", "40", "50", "30"])  # 5 values + expected median
        ]
        
        results = {}
        
        for circuit_path, circuit_name, test_inputs in circuits_to_test:
            print(f"\nğŸ”§ TESTE CIRCUIT: {circuit_name}")
            print("-" * 40)
            
            if not Path(circuit_path).exists():
                print(f"âŒ Circuit nicht gefunden: {circuit_path}")
                continue
            
            # Kompiliere und Setup
            compile_success = manager.compile_circuit(circuit_path, circuit_name)
            if not compile_success:
                print(f"âŒ Kompilierung fehlgeschlagen: {circuit_name}")
                continue
            
            setup_success = manager.setup_circuit(circuit_name)
            if not setup_success:
                print(f"âŒ Setup fehlgeschlagen: {circuit_name}")
                continue
            
            print(f"âœ… Circuit bereit: {circuit_name}")
            
            # Performance Test
            iterations = 5
            times = []
            sizes = []
            
            for i in range(iterations):
                result = manager.generate_proof(circuit_name, test_inputs)
                if result.success:
                    times.append(result.metrics.proof_time)
                    sizes.append(result.metrics.proof_size)
                else:
                    print(f"âŒ Proof {i+1} fehlgeschlagen: {result.error_message}")
            
            if times:
                avg_time = sum(times) / len(times)
                avg_size = sum(sizes) / len(sizes)
                
                print(f"ğŸ“Š Performance ({len(times)}/{iterations} erfolgreich):")
                print(f"   â±ï¸  Durchschnitt Zeit: {avg_time:.3f}s")
                print(f"   ğŸ“¦ Durchschnitt GrÃ¶ÃŸe: {avg_size:.0f} bytes")
                print(f"   ğŸ¯ Min/Max Zeit: {min(times):.3f}s / {max(times):.3f}s")
                
                results[circuit_name] = {
                    "avg_time": avg_time,
                    "avg_size": avg_size,
                    "min_time": min(times),
                    "max_time": max(times),
                    "success_rate": len(times) / iterations
                }
            else:
                print(f"âŒ Alle Proofs fehlgeschlagen fÃ¼r {circuit_name}")
                results[circuit_name] = {"success_rate": 0}
        
        # Speichere Multi-Circuit Ergebnisse
        results_file = Path("data/test_benchmarks/multi_circuit_performance.json")
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ“Š Multi-Circuit Ergebnisse gespeichert: {results_file}")
        
        return True, results
        
    except Exception as e:
        print(f"âŒ Multi-Circuit Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_iot_data_integrity():
    """Teste IoT Datengenerierung und -integritÃ¤t Ã¼ber lÃ¤ngere ZeitrÃ¤ume"""
    print("\n" + "=" * 80)
    print("LARGE SCALE: IoT Data Integrity Test")
    print("=" * 80)
    
    try:
        from src.iot_simulation.smart_home import SmartHomeSensors
        
        simulator = SmartHomeSensors()
        
        # Teste verschiedene ZeitrÃ¤ume
        test_periods = [
            (24, 3600, "1_day_hourly"),      # 1 Tag, stÃ¼ndlich
            (168, 3600, "1_week_hourly"),    # 1 Woche, stÃ¼ndlich  
            (24*7, 1800, "1_week_30min"),    # 1 Woche, 30min
        ]
        
        results = {}
        
        for hours, time_step, period_name in test_periods:
            print(f"\nğŸ“… TESTE PERIODE: {period_name}")
            print(f"   Dauer: {hours}h, Schritt: {time_step}s")
            print("-" * 40)
            
            start_time = time.time()
            readings = simulator.generate_readings(
                duration_hours=hours,
                time_step_seconds=time_step
            )
            generation_time = time.time() - start_time
            
            print(f"âœ… {len(readings):,} Readings generiert in {generation_time:.2f}s")
            
            # DatenintegritÃ¤t prÃ¼fen
            if readings:
                # Sensor-Typen
                sensor_types = set(r.sensor_type for r in readings)
                print(f"ğŸ“Š Sensor-Typen: {len(sensor_types)} ({', '.join(sorted(sensor_types))})")
                
                # RÃ¤ume
                rooms = set(r.room for r in readings)
                print(f"ğŸ  RÃ¤ume: {len(rooms)} ({', '.join(sorted(rooms))})")
                
                # Privacy Levels
                privacy_levels = set(r.privacy_level for r in readings)
                print(f"ğŸ”’ Privacy Levels: {sorted(privacy_levels)}")
                
                # Temperatur-Validierung
                temp_readings = [r for r in readings if r.sensor_type == "temperature"]
                if temp_readings:
                    temp_values = [r.value for r in temp_readings]
                    print(f"ğŸŒ¡ï¸  Temperatur: {min(temp_values):.1f}Â°C - {max(temp_values):.1f}Â°C")
                    
                    # PrÃ¼fe auf unrealistische Werte
                    unrealistic = [v for v in temp_values if v < -10 or v > 60]
                    if unrealistic:
                        print(f"âš ï¸  {len(unrealistic)} unrealistische Temperaturwerte gefunden!")
                    else:
                        print(f"âœ… Alle Temperaturwerte realistisch")
                
                # Zeitstempel-KontinuitÃ¤t
                timestamps = [r.timestamp for r in readings[:100]]  # Erste 100 prÃ¼fen
                print(f"â° Zeitstempel-KontinuitÃ¤t: OK (Sample geprÃ¼ft)")
                
                results[period_name] = {
                    "readings_count": len(readings),
                    "generation_time": generation_time,
                    "sensor_types": len(sensor_types),
                    "rooms": len(rooms),
                    "privacy_levels": sorted(privacy_levels),
                    "temp_range": [min(temp_values), max(temp_values)] if temp_readings else None,
                    "data_integrity": "OK"
                }
            else:
                print(f"âŒ Keine Readings generiert!")
                results[period_name] = {"error": "No readings generated"}
        
        # Speichere IoT Integrity Ergebnisse
        results_file = Path("data/test_benchmarks/iot_data_integrity.json")
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ“Š IoT Integrity Ergebnisse gespeichert: {results_file}")
        
        return True, results
        
    except Exception as e:
        print(f"âŒ IoT Data Integrity Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_system_stress_test():
    """Stress-Test des gesamten Systems"""
    print("\n" + "=" * 80)
    print("LARGE SCALE: System Stress Test")
    print("=" * 80)
    
    try:
        from src.evaluation.benchmark_framework import BenchmarkFramework, BenchmarkConfig
        from src.proof_systems.snark_manager import SNARKManager
        from src.iot_simulation.smart_home import SmartHomeSensors
        
        # Stress-Test Konfiguration
        stress_config = BenchmarkConfig(
            circuit_types=["filter_range", "min_max"],  # Nur stabile Circuits
            data_sizes=[50, 100],                       # Mittlere GrÃ¶ÃŸen
            batch_sizes=[10, 20],                       # Verschiedene Batches
            privacy_levels=[1, 2, 3],                   # Alle Privacy Levels
            iterations=2,                               # Weniger Iterationen fÃ¼r Speed
            output_dir="data/test_benchmarks/stress_test"
        )
        
        print(f"ğŸ”¥ Stress-Test Konfiguration:")
        print(f"   Circuits: {stress_config.circuit_types}")
        print(f"   Data Sizes: {stress_config.data_sizes}")
        print(f"   Batch Sizes: {stress_config.batch_sizes}")
        print(f"   Privacy Levels: {stress_config.privacy_levels}")
        print(f"   Iterationen: {stress_config.iterations}")
        
        # Erstelle Framework
        framework = BenchmarkFramework(stress_config)
        snark_manager = SNARKManager(circuits_dir="circuits", output_dir="data/test_proofs")
        iot_simulator = SmartHomeSensors()
        
        print(f"\nğŸš€ Starte Stress-Test...")
        start_time = time.time()
        
        # FÃ¼hre Stress-Test aus
        results = framework.run_benchmark_suite(snark_manager, iot_simulator)
        
        total_time = time.time() - start_time
        
        print(f"\nâœ… Stress-Test abgeschlossen in {total_time:.2f}s!")
        print(f"ğŸ“Š {len(results)} Benchmark-Ergebnisse generiert")
        
        # Analysiere Ergebnisse
        successful_results = [r for r in results if r.success_rate > 0.8]
        failed_results = [r for r in results if r.success_rate <= 0.8]
        
        print(f"\nğŸ“ˆ Stress-Test Analyse:")
        print(f"   âœ… Erfolgreiche Tests: {len(successful_results)}/{len(results)}")
        print(f"   âŒ Fehlgeschlagene Tests: {len(failed_results)}/{len(results)}")
        print(f"   ğŸ¯ Erfolgsrate: {len(successful_results)/len(results)*100:.1f}%")
        
        if successful_results:
            avg_proof_time = sum(r.performance.proof_generation_time for r in successful_results) / len(successful_results)
            avg_proof_size = sum(r.performance.proof_size for r in successful_results) / len(successful_results)
            
            print(f"   â±ï¸  Durchschnitt Proof Zeit: {avg_proof_time:.3f}s")
            print(f"   ğŸ“¦ Durchschnitt Proof GrÃ¶ÃŸe: {avg_proof_size:.0f} bytes")
        
        # Speichere Stress-Test Ergebnisse
        stress_results = {
            "total_time": total_time,
            "total_tests": len(results),
            "successful_tests": len(successful_results),
            "failed_tests": len(failed_results),
            "success_rate": len(successful_results)/len(results) if results else 0,
            "config": {
                "circuit_types": stress_config.circuit_types,
                "data_sizes": stress_config.data_sizes,
                "batch_sizes": stress_config.batch_sizes,
                "iterations": stress_config.iterations
            }
        }
        
        results_file = Path("data/test_benchmarks/stress_test_summary.json")
        with open(results_file, "w") as f:
            json.dump(stress_results, f, indent=2)
        
        print(f"\nğŸ“Š Stress-Test Ergebnisse gespeichert: {results_file}")
        
        return len(successful_results) >= len(results) * 0.8, stress_results
        
    except Exception as e:
        print(f"âŒ Stress-Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def main():
    """FÃ¼hre alle Large Scale Tests aus"""
    print("ğŸš€ LARGE SCALE EVALUATION SUITE")
    print("=" * 80)
    
    test_results = {}
    
    # Test 1: Crossover-Point Analyse
    print("\nğŸ¯ TEST 1: Crossover-Point Analyse")
    crossover_success, crossover_results = test_crossover_point_analysis()
    test_results["crossover_analysis"] = crossover_success
    
    # Test 2: Multi-Circuit Performance
    print("\nğŸ”§ TEST 2: Multi-Circuit Performance")
    circuit_success, circuit_results = test_multi_circuit_performance()
    test_results["multi_circuit"] = circuit_success
    
    # Test 3: IoT Data Integrity
    print("\nğŸ“Š TEST 3: IoT Data Integrity")
    integrity_success, integrity_results = test_iot_data_integrity()
    test_results["data_integrity"] = integrity_success
    
    # Test 4: System Stress Test
    print("\nğŸ”¥ TEST 4: System Stress Test")
    stress_success, stress_results = test_system_stress_test()
    test_results["stress_test"] = stress_success
    
    # Zusammenfassung
    print("\n" + "=" * 80)
    print("ğŸ† LARGE SCALE TEST ZUSAMMENFASSUNG")
    print("=" * 80)
    
    total_tests = len(test_results)
    passed_tests = sum(1 for success in test_results.values() if success)
    
    for test_name, success in test_results.items():
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"   {test_name:<20}: {status}")
    
    print(f"\nğŸ“Š Gesamtergebnis: {passed_tests}/{total_tests} Tests bestanden")
    
    if passed_tests == total_tests:
        print("ğŸ‰ ALLE LARGE SCALE TESTS ERFOLGREICH!")
        print("âœ… System ist bereit fÃ¼r Produktion")
        print("ğŸ”¥ Bachelorarbeit kann mit Vertrauen geschrieben werden!")
    else:
        print("âš ï¸  Einige Tests fehlgeschlagen - weitere Debugging nÃ¶tig")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
