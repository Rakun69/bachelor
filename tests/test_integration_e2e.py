#!/usr/bin/env python3
"""
Integration & End-to-End Tests
Testet das komplette System von IoT-Daten bis zu finalen Ergebnissen
"""

import sys
import os
import time
import json
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

def test_complete_orchestrator_workflow():
    """Teste den kompletten Orchestrator Workflow"""
    print("=" * 80)
    print("INTEGRATION: Kompletter Orchestrator Workflow")
    print("=" * 80)
    
    try:
        from src.orchestrator import IoTZKOrchestrator
        
        # Erstelle Orchestrator mit Test-Config
        test_config = {
            "iot_simulation": {
                "multi_period_enabled": False,  # FÃ¼r schnelleren Test
                "duration_hours": 2,            # Nur 2 Stunden
                "time_step_seconds": 300        # 5 Minuten Schritte
            },
            "circuit_types": ["filter_range"],  # Nur ein Circuit
            "data_sizes": [10, 20],            # Kleine GrÃ¶ÃŸen
            "batch_sizes": [5],                # Eine Batch-GrÃ¶ÃŸe
            "privacy_levels": [2],             # Ein Privacy Level
            "iterations": 1,                   # Eine Iteration
            "evaluation": {
                "run_performance_tests": True,
                "run_privacy_analysis": True,
                "run_scalability_tests": True,
                "generate_visualizations": False,  # FÃ¼r Speed
                "run_nova_comparison": False       # FÃ¼r Speed
            }
        }
        
        # Speichere Test-Config
        config_file = Path("data/test_config.json")
        config_file.parent.mkdir(parents=True, exist_ok=True)
        with open(config_file, "w") as f:
            json.dump(test_config, f, indent=2)
        
        print(f"ğŸ“ Test-Konfiguration erstellt: {config_file}")
        
        # Erstelle Orchestrator
        orchestrator = IoTZKOrchestrator(str(config_file))
        
        print(f"âœ… Orchestrator initialisiert")
        
        # Teste Phase fÃ¼r Phase
        phases_results = {}
        
        # Phase 1: Datengenerierung
        print(f"\nğŸ“Š Phase 1: IoT Datengenerierung")
        iot_data = orchestrator._generate_iot_data()
        
        if isinstance(iot_data, dict) and iot_data.get("total_readings", 0) > 0:
            print(f"âœ… IoT Daten generiert: {iot_data['total_readings']} readings")
            phases_results["data_generation"] = True
        else:
            print(f"âŒ IoT Datengenerierung fehlgeschlagen")
            phases_results["data_generation"] = False
            return False, phases_results
        
        # Phase 2: Circuit Kompilierung
        print(f"\nğŸ”§ Phase 2: Circuit Kompilierung")
        circuit_status = orchestrator._compile_circuits()
        
        successful_circuits = sum(1 for status in circuit_status.values() 
                                if status.get("status") == "success")
        
        if successful_circuits > 0:
            print(f"âœ… {successful_circuits} Circuits erfolgreich kompiliert")
            phases_results["circuit_compilation"] = True
        else:
            print(f"âŒ Circuit Kompilierung fehlgeschlagen")
            phases_results["circuit_compilation"] = False
            return False, phases_results
        
        # Phase 3: Benchmarks
        print(f"\nğŸ“ˆ Phase 3: Benchmark AusfÃ¼hrung")
        benchmark_results = orchestrator._run_benchmarks()
        
        if isinstance(benchmark_results, list) and len(benchmark_results) > 0:
            successful_benchmarks = sum(1 for r in benchmark_results 
                                      if r.get("success_rate", 0) > 0.5)
            print(f"âœ… {successful_benchmarks}/{len(benchmark_results)} Benchmarks erfolgreich")
            phases_results["benchmarks"] = successful_benchmarks > 0
        else:
            print(f"âŒ Benchmark AusfÃ¼hrung fehlgeschlagen")
            phases_results["benchmarks"] = False
            return False, phases_results
        
        # Phase 4: Analyse
        print(f"\nğŸ” Phase 4: Ergebnis-Analyse")
        analysis = orchestrator._analyze_results()
        
        if isinstance(analysis, dict) and "comparison_report" in analysis:
            print(f"âœ… Analyse erfolgreich durchgefÃ¼hrt")
            phases_results["analysis"] = True
        else:
            print(f"âŒ Analyse fehlgeschlagen")
            phases_results["analysis"] = False
        
        # Zusammenfassung
        successful_phases = sum(1 for success in phases_results.values() if success)
        total_phases = len(phases_results)
        
        print(f"\nğŸ“Š Workflow Zusammenfassung:")
        print(f"   Erfolgreiche Phasen: {successful_phases}/{total_phases}")
        
        return successful_phases == total_phases, phases_results
        
    except Exception as e:
        print(f"âŒ Orchestrator Workflow fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False, {"error": str(e)}

def test_data_pipeline_integrity():
    """Teste die DatenintegritÃ¤t durch die gesamte Pipeline"""
    print("\n" + "=" * 80)
    print("INTEGRATION: Daten-Pipeline IntegritÃ¤t")
    print("=" * 80)
    
    try:
        from src.iot_simulation.smart_home import SmartHomeSensors
        from src.proof_systems.snark_manager import SNARKManager
        
        # Schritt 1: IoT Daten generieren
        print("ğŸ“Š Schritt 1: IoT Daten generieren")
        simulator = SmartHomeSensors()
        readings = simulator.generate_readings(duration_hours=1, time_step_seconds=300)
        
        if not readings:
            print("âŒ Keine IoT Daten generiert")
            return False
        
        print(f"âœ… {len(readings)} IoT Readings generiert")
        
        # Schritt 2: Daten fÃ¼r ZK-Proofs vorbereiten
        print("ğŸ”§ Schritt 2: Daten fÃ¼r ZK-Proofs vorbereiten")
        
        # Filtere Temperatur-Daten
        temp_readings = [r for r in readings if r.sensor_type == "temperature"]
        
        if not temp_readings:
            print("âŒ Keine Temperatur-Daten gefunden")
            return False
        
        print(f"âœ… {len(temp_readings)} Temperatur-Readings gefiltert")
        
        # Schritt 3: ZK-Proofs generieren
        print("ğŸ” Schritt 3: ZK-Proofs generieren")
        
        manager = SNARKManager(circuits_dir="circuits", output_dir="data/test_proofs")
        
        # Kompiliere filter_range Circuit
        circuit_path = Path("circuits/basic/filter_range.zok")
        if not manager.compile_circuit(str(circuit_path), "filter_range"):
            print("âŒ Circuit Kompilierung fehlgeschlagen")
            return False
        
        if not manager.setup_circuit("filter_range"):
            print("âŒ Circuit Setup fehlgeschlagen")
            return False
        
        # Generiere Proofs fÃ¼r erste 5 Temperatur-Readings
        successful_proofs = 0
        total_proof_size = 0
        
        for i, reading in enumerate(temp_readings[:5]):
            temp_value = int(reading.value)
            inputs = ["10", "40", str(temp_value)]  # Min: 10Â°C, Max: 40Â°C
            
            result = manager.generate_proof("filter_range", inputs)
            
            if result.success:
                successful_proofs += 1
                total_proof_size += result.metrics.proof_size
                print(f"   âœ… Proof {i+1}: {temp_value}Â°C -> {result.metrics.proof_size} bytes")
            else:
                print(f"   âŒ Proof {i+1} fehlgeschlagen: {result.error_message}")
        
        print(f"âœ… {successful_proofs}/5 Proofs erfolgreich generiert")
        print(f"ğŸ“¦ Gesamt Proof-GrÃ¶ÃŸe: {total_proof_size:,} bytes")
        
        # Schritt 4: DatenintegritÃ¤t prÃ¼fen
        print("ğŸ” Schritt 4: DatenintegritÃ¤t prÃ¼fen")
        
        # PrÃ¼fe ob alle Temperaturwerte im erwarteten Bereich sind
        temp_values = [r.value for r in temp_readings]
        valid_temps = [v for v in temp_values if 10 <= v <= 40]
        
        integrity_score = len(valid_temps) / len(temp_values) if temp_values else 0
        
        print(f"ğŸŒ¡ï¸  Temperatur-IntegritÃ¤t: {len(valid_temps)}/{len(temp_values)} Werte im Bereich 10-40Â°C")
        print(f"ğŸ“Š IntegritÃ¤ts-Score: {integrity_score:.2%}")
        
        # Pipeline erfolgreich wenn > 80% der Daten valide sind und > 80% der Proofs erfolgreich
        pipeline_success = (integrity_score > 0.8 and 
                          successful_proofs >= 4)  # 4/5 = 80%
        
        if pipeline_success:
            print("âœ… Daten-Pipeline IntegritÃ¤t bestÃ¤tigt")
        else:
            print("âŒ Daten-Pipeline IntegritÃ¤t unzureichend")
        
        return pipeline_success
        
    except Exception as e:
        print(f"âŒ Daten-Pipeline Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_error_handling_robustness():
    """Teste Fehlerbehandlung und Robustheit des Systems"""
    print("\n" + "=" * 80)
    print("INTEGRATION: Error Handling & Robustheit")
    print("=" * 80)
    
    try:
        from src.proof_systems.snark_manager import SNARKManager
        
        manager = SNARKManager(circuits_dir="circuits", output_dir="data/test_proofs")
        
        error_tests = []
        
        # Test 1: Nicht-existierender Circuit
        print("ğŸ§ª Test 1: Nicht-existierender Circuit")
        try:
            result = manager.compile_circuit("non_existent_circuit.zok", "fake_circuit")
            if not result:
                print("âœ… Fehler korrekt abgefangen: Nicht-existierender Circuit")
                error_tests.append(True)
            else:
                print("âŒ Fehler nicht abgefangen: Nicht-existierender Circuit")
                error_tests.append(False)
        except Exception as e:
            print("âŒ Unbehandelte Exception bei nicht-existierendem Circuit")
            error_tests.append(False)
        
        # Test 2: UngÃ¼ltige Proof-Inputs
        print("ğŸ§ª Test 2: UngÃ¼ltige Proof-Inputs")
        
        # Kompiliere filter_range fÃ¼r Test
        circuit_path = Path("circuits/basic/filter_range.zok")
        if circuit_path.exists():
            manager.compile_circuit(str(circuit_path), "filter_range")
            manager.setup_circuit("filter_range")
            
            try:
                # UngÃ¼ltige Inputs: secret_value auÃŸerhalb des Bereichs
                invalid_inputs = ["10", "20", "30"]  # 30 ist > 20 (max_val)
                result = manager.generate_proof("filter_range", invalid_inputs)
                
                if not result.success:
                    print("âœ… UngÃ¼ltige Inputs korrekt abgelehnt")
                    error_tests.append(True)
                else:
                    print("âŒ UngÃ¼ltige Inputs nicht erkannt")
                    error_tests.append(False)
            except Exception as e:
                print("âŒ Unbehandelte Exception bei ungÃ¼ltigen Inputs")
                error_tests.append(False)
        else:
            print("âš ï¸  Circuit nicht gefunden - Test Ã¼bersprungen")
            error_tests.append(True)  # Nicht als Fehler werten
        
        # Test 3: Speicher-/Timeout-Robustheit
        print("ğŸ§ª Test 3: Timeout-Robustheit")
        
        # Teste mit sehr groÃŸen Arrays (sollte Timeout auslÃ¶sen oder graceful handhaben)
        try:
            # Erstelle sehr lange Input-Liste
            large_inputs = [str(i) for i in range(1000)]  # Viel zu viele Inputs
            
            result = manager.generate_proof("filter_range", large_inputs)
            
            # Sollte entweder fehlschlagen oder Timeout
            if not result.success:
                print("âœ… GroÃŸe Inputs korrekt behandelt (Fehler erwartet)")
                error_tests.append(True)
            else:
                print("âš ï¸  GroÃŸe Inputs akzeptiert (unerwartet aber nicht kritisch)")
                error_tests.append(True)
        except Exception as e:
            print("âœ… Exception bei groÃŸen Inputs korrekt abgefangen")
            error_tests.append(True)
        
        # Test 4: IoT Simulator Robustheit
        print("ğŸ§ª Test 4: IoT Simulator Robustheit")
        
        try:
            from src.iot_simulation.smart_home import SmartHomeSensors
            
            simulator = SmartHomeSensors()
            
            # Test mit extremen Parametern
            readings = simulator.generate_readings(duration_hours=0, time_step_seconds=1)
            
            if isinstance(readings, list):
                print("âœ… IoT Simulator behandelt extreme Parameter graceful")
                error_tests.append(True)
            else:
                print("âŒ IoT Simulator Fehler bei extremen Parametern")
                error_tests.append(False)
        except Exception as e:
            print("âŒ Unbehandelte Exception im IoT Simulator")
            error_tests.append(False)
        
        # Zusammenfassung
        passed_error_tests = sum(error_tests)
        total_error_tests = len(error_tests)
        
        print(f"\nğŸ“Š Error Handling Zusammenfassung:")
        print(f"   Bestandene Tests: {passed_error_tests}/{total_error_tests}")
        
        robustness_score = passed_error_tests / total_error_tests if total_error_tests > 0 else 0
        
        if robustness_score >= 0.8:
            print("âœ… System ist robust und behandelt Fehler korrekt")
            return True
        else:
            print("âŒ System-Robustheit unzureichend")
            return False
        
    except Exception as e:
        print(f"âŒ Error Handling Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance_consistency():
    """Teste Performance-Konsistenz Ã¼ber mehrere DurchlÃ¤ufe"""
    print("\n" + "=" * 80)
    print("INTEGRATION: Performance Konsistenz")
    print("=" * 80)
    
    try:
        from src.proof_systems.snark_manager import SNARKManager
        
        manager = SNARKManager(circuits_dir="circuits", output_dir="data/test_proofs")
        
        # Setup filter_range Circuit
        circuit_path = Path("circuits/basic/filter_range.zok")
        if not circuit_path.exists():
            print("âŒ Circuit nicht gefunden")
            return False
        
        manager.compile_circuit(str(circuit_path), "filter_range")
        manager.setup_circuit("filter_range")
        
        # Performance Test Ã¼ber mehrere DurchlÃ¤ufe
        iterations = 10
        test_inputs = ["10", "50", "25"]
        
        times = []
        sizes = []
        
        print(f"ğŸ”„ FÃ¼hre {iterations} Performance-Tests durch...")
        
        for i in range(iterations):
            result = manager.generate_proof("filter_range", test_inputs)
            
            if result.success:
                times.append(result.metrics.proof_time)
                sizes.append(result.metrics.proof_size)
                print(f"   Test {i+1}: {result.metrics.proof_time:.3f}s, {result.metrics.proof_size} bytes")
            else:
                print(f"   Test {i+1}: âŒ Fehlgeschlagen")
        
        if len(times) < iterations * 0.8:  # Mindestens 80% erfolgreich
            print("âŒ Zu viele fehlgeschlagene Tests fÃ¼r Konsistenz-Analyse")
            return False
        
        # Statistiken berechnen
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        time_variance = sum((t - avg_time) ** 2 for t in times) / len(times)
        time_std_dev = time_variance ** 0.5
        
        avg_size = sum(sizes) / len(sizes)
        size_variance = sum((s - avg_size) ** 2 for s in sizes) / len(sizes)
        
        print(f"\nğŸ“Š Performance Statistiken:")
        print(f"   â±ï¸  Zeit - Durchschnitt: {avg_time:.3f}s")
        print(f"   â±ï¸  Zeit - Min/Max: {min_time:.3f}s / {max_time:.3f}s")
        print(f"   â±ï¸  Zeit - Std. Abweichung: {time_std_dev:.3f}s")
        print(f"   ğŸ“¦ GrÃ¶ÃŸe - Durchschnitt: {avg_size:.0f} bytes")
        print(f"   ğŸ“¦ GrÃ¶ÃŸe - Varianz: {size_variance:.0f}")
        
        # Konsistenz-Bewertung
        time_consistency = (time_std_dev / avg_time) < 0.2  # < 20% Variation
        size_consistency = size_variance == 0  # Proof-GrÃ¶ÃŸe sollte konstant sein
        
        print(f"\nğŸ¯ Konsistenz-Bewertung:")
        print(f"   Zeit-Konsistenz: {'âœ…' if time_consistency else 'âŒ'} ({'OK' if time_consistency else 'Zu variabel'})")
        print(f"   GrÃ¶ÃŸe-Konsistenz: {'âœ…' if size_consistency else 'âŒ'} ({'Konstant' if size_consistency else 'Variabel'})")
        
        overall_consistency = time_consistency and size_consistency
        
        if overall_consistency:
            print("âœ… Performance ist konsistent Ã¼ber mehrere DurchlÃ¤ufe")
        else:
            print("âš ï¸  Performance-Inkonsistenzen erkannt")
        
        return overall_consistency
        
    except Exception as e:
        print(f"âŒ Performance Konsistenz Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """FÃ¼hre alle Integration & E2E Tests aus"""
    print("ğŸš€ INTEGRATION & END-TO-END TEST SUITE")
    print("=" * 80)
    
    test_results = {}
    
    # Test 1: Kompletter Orchestrator Workflow
    print("ğŸ¯ TEST 1: Kompletter Orchestrator Workflow")
    workflow_success, workflow_details = test_complete_orchestrator_workflow()
    test_results["orchestrator_workflow"] = workflow_success
    
    # Test 2: Daten-Pipeline IntegritÃ¤t
    print("\nğŸ“Š TEST 2: Daten-Pipeline IntegritÃ¤t")
    pipeline_success = test_data_pipeline_integrity()
    test_results["data_pipeline"] = pipeline_success
    
    # Test 3: Error Handling & Robustheit
    print("\nğŸ›¡ï¸  TEST 3: Error Handling & Robustheit")
    robustness_success = test_error_handling_robustness()
    test_results["error_handling"] = robustness_success
    
    # Test 4: Performance Konsistenz
    print("\nğŸ“ˆ TEST 4: Performance Konsistenz")
    consistency_success = test_performance_consistency()
    test_results["performance_consistency"] = consistency_success
    
    # Zusammenfassung
    print("\n" + "=" * 80)
    print("ğŸ† INTEGRATION & E2E TEST ZUSAMMENFASSUNG")
    print("=" * 80)
    
    total_tests = len(test_results)
    passed_tests = sum(1 for success in test_results.values() if success)
    
    for test_name, success in test_results.items():
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"   {test_name:<25}: {status}")
    
    print(f"\nğŸ“Š Gesamtergebnis: {passed_tests}/{total_tests} Tests bestanden")
    
    if passed_tests == total_tests:
        print("ğŸ‰ ALLE INTEGRATION TESTS ERFOLGREICH!")
        print("âœ… System ist vollstÃ¤ndig integriert und funktionsfÃ¤hig")
        print("ğŸ”¥ End-to-End Pipeline funktioniert einwandfrei")
    else:
        print("âš ï¸  Einige Integration-Tests fehlgeschlagen")
        print("ğŸ”§ System braucht weitere Integration-Arbeit")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
